{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e73983d5-a79e-4822-b1d7-5908e71d51e1",
   "metadata": {},
   "source": [
    "# BERTGCN 改良加上 text segment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c121adfa-afec-4fb1-92c8-804a9314bfe8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10230.400000000001"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "df=pd.read_csv(\"../data/book.csv\",names=['label','text'])\n",
    "len(df['label'])*0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b685e9a2-cd4e-4cd1-b61f-3ae5e9bd9b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd \n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# data= pd.read_csv(\"../dataset/IMDB/IMDB_over512.csv\", names = [\"label\", \"text\"])\n",
    "# LE = LabelEncoder()\n",
    "# data['label'] = LE.fit_transform(data['label'])\n",
    "# train_data, test_data = train_test_split(data, random_state=777, train_size=0.5)\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset, DatasetDict\n",
    "from sklearn.model_selection import train_test_split\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "df=pd.read_csv(\"../data/book.csv\",names=['label','text'])\n",
    "train_data, test_data = train_test_split(df, test_size=0.2, shuffle = False)\n",
    "label = []\n",
    "for index in range(len(df['label'])): \n",
    "     label.append(eval(df['label'][index]))\n",
    "vector = np.vectorize(np.int_)\n",
    "label = np.array(label)\n",
    "label = vector(label)\n",
    "train = df.copy()\n",
    "# train_data=Dataset.from_pandas(train_data)\n",
    "# test_data=Dataset.from_pandas(test_data)\n",
    "\n",
    "# print(unique_labels)\n",
    "# print(lEnc.transform(unique_labels))\n",
    "\n",
    "# train_labels = label[:10230]\n",
    "# test_labels = label[10230:]\n",
    "train_labels = label[:10230]\n",
    "test_labels = label[10230:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c54d24d-03c9-479a-ab54-4ffacbd9fc5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "#dataset_name = 'own'\n",
    "#sentences = ['Would you like a plain sweater or something else?​', 'Great. We have some very nice wool slacks over here. Would you like to take a look?']\n",
    "#labels = ['Yes' , 'No' ]\n",
    "#train_or_test_list = ['train', 'test']\n",
    "import pandas as pd\n",
    "data= pd.read_csv(\"../data/book.csv\",names=['label','text'])\n",
    "# data['label'] = LE.fit_transform(data['label'])\n",
    "sentences = []\n",
    "labels = []\n",
    "label = []\n",
    "for index in range(len(df['label'])): \n",
    "     label.append(eval(df['label'][index]))\n",
    "vector = np.vectorize(np.int_)\n",
    "label = np.array(label)\n",
    "labels = vector(label)\n",
    "\n",
    "\n",
    "train_or_test_list = []\n",
    "for index,  label,text in data.itertuples():\n",
    "  sentences.append(text)\n",
    "  # labels.append(label)\n",
    "  train_or_test_list.append('train' if index < 10230 else 'test')\n",
    "dataset_name = 'book'\n",
    "\n",
    "\n",
    "meta_data_list = []\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    meta = str(i) + '\\t' + train_or_test_list[i] + '\\t' + str(labels[i])\n",
    "    meta_data_list.append(meta)\n",
    "\n",
    "meta_data_str = '\\n'.join(meta_data_list)\n",
    "\n",
    "f = open( 'data/ '+ dataset_name + '.txt', 'w')\n",
    "f.write(meta_data_str)\n",
    "f.close()\n",
    "\n",
    "#corpus_str = '\\n'.join(sentences)\n",
    "corpus_str = '\\n'.join([str(i) for i in sentences])\n",
    "\n",
    "f = open('data/corpus/' + dataset_name + '.txt', 'w')\n",
    "f.write(corpus_str)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c726f5-ebef-4bd9-bfea-76838ce94f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import networkx as nx\n",
    "import scipy.sparse as sp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import log\n",
    "from transformers import AutoTokenizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "dataset = 'book'\n",
    "data= pd.read_csv(\"../data/book.csv\",names=['label','text'])\n",
    "LE = LabelEncoder()\n",
    "# data['label'] = LE.fit_transform(data['label'])\n",
    "sentences = []\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"klue/roberta-base\")\n",
    "\n",
    "word_embeddings_dim = 300\n",
    "word_vector_map = {}\n",
    "\n",
    "doc_name_list = []\n",
    "doc_train_list = []\n",
    "doc_test_list = []\n",
    "\n",
    "f = open('data/' + dataset + '.txt', 'r', encoding=\"utf-8\")\n",
    "lines = f.readlines()\n",
    "for line in lines:\n",
    "    doc_name_list.append(line.strip())\n",
    "    temp = line.split(\"\\t\")\n",
    "    if temp[1].find('test') != -1:\n",
    "        doc_test_list.append(line.strip())\n",
    "    elif temp[1].find('train') != -1:\n",
    "        doc_train_list.append(line.strip())\n",
    "f.close()\n",
    "\n",
    "doc_content_list = []\n",
    "f = open('data/corpus/' + dataset + '.txt', 'r', encoding=\"utf-8\")\n",
    "lines = f.readlines()\n",
    "for line in lines:\n",
    "    doc_content_list.append(line.strip())\n",
    "f.close()\n",
    "\n",
    "train_ids = []\n",
    "for train_name in doc_train_list:\n",
    "    train_id = doc_name_list.index(train_name)\n",
    "    train_ids.append(train_id)\n",
    "# random.shuffle(train_ids)\n",
    "\n",
    "train_ids_str = '\\n'.join(str(index) for index in train_ids)\n",
    "f = open('data/' + dataset + '.train.index', 'w')\n",
    "f.write(train_ids_str)\n",
    "f.close()\n",
    "\n",
    "test_ids = []\n",
    "for test_name in doc_test_list:\n",
    "    test_id = doc_name_list.index(test_name)\n",
    "    test_ids.append(test_id)\n",
    "# random.shuffle(test_ids)\n",
    "\n",
    "test_ids_str = '\\n'.join(str(index) for index in test_ids)\n",
    "f = open('data/' + dataset + '.test.index', 'w')\n",
    "f.write(test_ids_str)\n",
    "f.close()\n",
    "\n",
    "shuffle_doc_name_list = []\n",
    "shuffle_doc_words_list = []\n",
    "ids = train_ids + test_ids\n",
    "for id in ids:\n",
    "    shuffle_doc_name_list.append(doc_name_list[int(id)])\n",
    "    shuffle_doc_words_list.append(doc_content_list[int(id)])\n",
    "shuffle_doc_name_str = '\\n'.join(shuffle_doc_name_list)\n",
    "shuffle_doc_words_str = '\\n'.join(shuffle_doc_words_list)\n",
    "\n",
    "f = open('data/' + dataset + '_shuffle.txt', 'w', encoding=\"utf-8\")\n",
    "f.write(shuffle_doc_name_str)\n",
    "f.close()\n",
    "\n",
    "f = open('data/corpus/' + dataset + '_shuffle.txt', 'w', encoding=\"utf-8\")\n",
    "f.write(shuffle_doc_words_str)\n",
    "f.close()\n",
    "\n",
    "word_freq = {}\n",
    "word_set = set()\n",
    "for doc_words in shuffle_doc_words_list:\n",
    "    words = tokenizer.tokenize(doc_words)\n",
    "    for word in words:\n",
    "        word_set.add(word)\n",
    "        if word in word_freq:\n",
    "            word_freq[word] += 1\n",
    "        else:\n",
    "            word_freq[word] = 1\n",
    "\n",
    "vocab = list(word_set)\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "word_doc_list = {}\n",
    "\n",
    "for i in range(len(shuffle_doc_words_list)):\n",
    "    doc_words = shuffle_doc_words_list[i]\n",
    "    words = tokenizer.tokenize(doc_words)\n",
    "    appeared = set()\n",
    "    for word in words:\n",
    "        if word in appeared:\n",
    "            continue\n",
    "        if word in word_doc_list:\n",
    "            doc_list = word_doc_list[word]\n",
    "            doc_list.append(i)\n",
    "            word_doc_list[word] = doc_list\n",
    "        else:\n",
    "            word_doc_list[word] = [i]\n",
    "        appeared.add(word)\n",
    "\n",
    "word_doc_freq = {}\n",
    "for word, doc_list in word_doc_list.items():\n",
    "    word_doc_freq[word] = len(doc_list)\n",
    "\n",
    "word_id_map = {}\n",
    "for i in range(vocab_size):\n",
    "    word_id_map[vocab[i]] = i\n",
    "\n",
    "vocab_str = '\\n'.join(vocab)\n",
    "\n",
    "f = open('data/corpus/' + dataset + '_vocab.txt', 'w',encoding=\"utf-8\")\n",
    "f.write(vocab_str)\n",
    "f.close()\n",
    "\n",
    "label_set = set()\n",
    "for doc_meta in shuffle_doc_name_list:\n",
    "    temp = doc_meta.split('\\t')\n",
    "    label_set.add(temp[2])\n",
    "label_list = list(label_set)\n",
    "\n",
    "label_list_str = '\\n'.join(label_list)\n",
    "f = open('data/corpus/' + dataset + '_labels.txt', 'w',encoding=\"utf-8\")\n",
    "f.write(label_list_str)\n",
    "f.close()\n",
    "\n",
    "train_size = len(train_ids)\n",
    "val_size = int(0.1 * train_size)\n",
    "real_train_size = train_size - val_size\n",
    "\n",
    "real_train_doc_names = shuffle_doc_name_list[:real_train_size]\n",
    "real_train_doc_names_str = '\\n'.join(real_train_doc_names)\n",
    "\n",
    "f = open('data/' + dataset + '.real_train.name', 'w',encoding=\"utf-8\")\n",
    "f.write(real_train_doc_names_str)\n",
    "f.close()\n",
    "\n",
    "row_x = []\n",
    "col_x = []\n",
    "data_x = []\n",
    "\n",
    "for i in range(real_train_size):\n",
    "    doc_vec = np.array([0.0 for k in range(word_embeddings_dim)])\n",
    "    doc_words = shuffle_doc_words_list[i]\n",
    "    words = tokenizer.tokenize(doc_words)\n",
    "    doc_len = len(words)\n",
    "    for word in words:\n",
    "        if word in word_vector_map:\n",
    "            word_vector = word_vector_map[word]\n",
    "            doc_vec = doc_vec + np.array(word_vector)\n",
    "\n",
    "    for j in range(word_embeddings_dim):\n",
    "        row_x.append(i)\n",
    "        col_x.append(j)\n",
    "        data_x.append(doc_vec[j] / doc_len)\n",
    "        \n",
    "x = sp.csr_matrix((data_x, (row_x, col_x)), shape=(\n",
    "    real_train_size, word_embeddings_dim))\n",
    "\n",
    "y = []\n",
    "for i in range(real_train_size):\n",
    "    doc_meta = shuffle_doc_name_list[i]\n",
    "    temp = doc_meta.split('\\t')\n",
    "    label = temp[2]\n",
    "    one_hot = [0 for l in range(len(label_list))]\n",
    "    label_index = label_list.index(label)\n",
    "    one_hot[label_index] = 1\n",
    "    y.append(one_hot)\n",
    "y = np.array(y)\n",
    "\n",
    "test_size = len(test_ids)\n",
    "\n",
    "row_tx = []\n",
    "col_tx = []\n",
    "data_tx = []\n",
    "for i in range(test_size):\n",
    "    doc_vec = np.array([0.0 for k in range(word_embeddings_dim)])\n",
    "    doc_words = shuffle_doc_words_list[i + train_size]\n",
    "    words = tokenizer.tokenize(doc_words)\n",
    "    doc_len = len(words)\n",
    "    for word in words:\n",
    "        if word in word_vector_map:\n",
    "            word_vector = word_vector_map[word]\n",
    "            doc_vec = doc_vec + np.array(word_vector)\n",
    "\n",
    "    for j in range(word_embeddings_dim):\n",
    "        row_tx.append(i)\n",
    "        col_tx.append(j)\n",
    "        data_tx.append(doc_vec[j] / doc_len)\n",
    "\n",
    "tx = sp.csr_matrix((data_tx, (row_tx, col_tx)),\n",
    "                   shape=(test_size, word_embeddings_dim))\n",
    "\n",
    "ty = []\n",
    "for i in range(test_size):\n",
    "    doc_meta = shuffle_doc_name_list[i + train_size]\n",
    "    temp = doc_meta.split('\\t')\n",
    "    label = temp[2]\n",
    "    one_hot = [0 for l in range(len(label_list))]\n",
    "    label_index = label_list.index(label)\n",
    "    one_hot[label_index] = 1\n",
    "    ty.append(one_hot)\n",
    "ty = np.array(ty)\n",
    "\n",
    "word_vectors = np.random.uniform(-0.01, 0.01, (vocab_size, word_embeddings_dim))\n",
    "\n",
    "for i in range(len(vocab)):\n",
    "    word = vocab[i]\n",
    "    if word in word_vector_map:\n",
    "        vector = word_vector_map[word]\n",
    "        word_vectors[i] = vector\n",
    "\n",
    "row_allx = []\n",
    "col_allx = []\n",
    "data_allx = []\n",
    "\n",
    "for i in range(train_size):\n",
    "    doc_vec = np.array([0.0 for k in range(word_embeddings_dim)])\n",
    "    doc_words = shuffle_doc_words_list[i]\n",
    "    words = tokenizer.tokenize(doc_words)\n",
    "    doc_len = len(words)\n",
    "    for word in words:\n",
    "        if word in word_vector_map:\n",
    "            word_vector = word_vector_map[word]\n",
    "            doc_vec = doc_vec + np.array(word_vector)\n",
    "\n",
    "    for j in range(word_embeddings_dim):\n",
    "        row_allx.append(int(i))\n",
    "        col_allx.append(j)\n",
    "        data_allx.append(doc_vec[j] / doc_len)\n",
    "for i in range(vocab_size):\n",
    "    for j in range(word_embeddings_dim):\n",
    "        row_allx.append(int(i + train_size))\n",
    "        col_allx.append(j)\n",
    "        data_allx.append(word_vectors.item((i, j)))\n",
    "\n",
    "\n",
    "row_allx = np.array(row_allx)\n",
    "col_allx = np.array(col_allx)\n",
    "data_allx = np.array(data_allx)\n",
    "\n",
    "allx = sp.csr_matrix(\n",
    "    (data_allx, (row_allx, col_allx)), shape=(train_size + vocab_size, word_embeddings_dim))\n",
    "\n",
    "ally = []\n",
    "for i in range(train_size):\n",
    "    doc_meta = shuffle_doc_name_list[i]\n",
    "    temp = doc_meta.split('\\t')\n",
    "    label = temp[2]\n",
    "    one_hot = [0 for l in range(len(label_list))]\n",
    "    label_index = label_list.index(label)\n",
    "    one_hot[label_index] = 1\n",
    "    ally.append(one_hot)\n",
    "\n",
    "for i in range(vocab_size):\n",
    "    one_hot = [0 for l in range(len(label_list))]\n",
    "    ally.append(one_hot)\n",
    "\n",
    "ally = np.array(ally)\n",
    "\n",
    "print(x.shape, y.shape, tx.shape, ty.shape, allx.shape, ally.shape)\n",
    "\n",
    "window_size = 20\n",
    "windows = []\n",
    "\n",
    "for doc_words in shuffle_doc_words_list:\n",
    "    words = tokenizer.tokenize(doc_words)\n",
    "    length = len(words)\n",
    "    if length <= window_size:\n",
    "        windows.append(words)\n",
    "    else:\n",
    "        for j in range(length - window_size + 1):\n",
    "            window = words[j: j + window_size]\n",
    "            windows.append(window)\n",
    "\n",
    "\n",
    "word_window_freq = {}\n",
    "for window in windows:\n",
    "    appeared = set()\n",
    "    for i in range(len(window)):\n",
    "        if window[i] in appeared:\n",
    "            continue\n",
    "        if window[i] in word_window_freq:\n",
    "            word_window_freq[window[i]] += 1\n",
    "        else:\n",
    "            word_window_freq[window[i]] = 1\n",
    "        appeared.add(window[i])\n",
    "\n",
    "word_pair_count = {}\n",
    "for window in windows:\n",
    "    for i in range(1, len(window)):\n",
    "        for j in range(0, i):\n",
    "            word_i = window[i]\n",
    "            word_i_id = word_id_map[word_i]\n",
    "            word_j = window[j]\n",
    "            word_j_id = word_id_map[word_j]\n",
    "            if word_i_id == word_j_id:\n",
    "                continue\n",
    "            word_pair_str = str(word_i_id) + ',' + str(word_j_id)\n",
    "            if word_pair_str in word_pair_count:\n",
    "                word_pair_count[word_pair_str] += 1\n",
    "            else:\n",
    "                word_pair_count[word_pair_str] = 1\n",
    "            # two orders\n",
    "            word_pair_str = str(word_j_id) + ',' + str(word_i_id)\n",
    "            if word_pair_str in word_pair_count:\n",
    "                word_pair_count[word_pair_str] += 1\n",
    "            else:\n",
    "                word_pair_count[word_pair_str] = 1\n",
    "\n",
    "row = []\n",
    "col = []\n",
    "weight = []\n",
    "\n",
    "num_window = len(windows)\n",
    "\n",
    "for key in word_pair_count:\n",
    "    temp = key.split(',')\n",
    "    i = int(temp[0])\n",
    "    j = int(temp[1])\n",
    "    count = word_pair_count[key]\n",
    "    word_freq_i = word_window_freq[vocab[i]]\n",
    "    word_freq_j = word_window_freq[vocab[j]]\n",
    "    pmi = log((1.0 * count / num_window) /\n",
    "              (1.0 * word_freq_i * word_freq_j/(num_window * num_window)))\n",
    "    if pmi <= 0:\n",
    "        continue\n",
    "    row.append(train_size + i)\n",
    "    col.append(train_size + j)\n",
    "    weight.append(pmi)\n",
    "\n",
    "doc_word_freq = {}\n",
    "\n",
    "for doc_id in range(len(shuffle_doc_words_list)):\n",
    "    doc_words = shuffle_doc_words_list[doc_id]\n",
    "    words = tokenizer.tokenize(doc_words)\n",
    "    for word in words:\n",
    "        word_id = word_id_map[word]\n",
    "        doc_word_str = str(doc_id) + ',' + str(word_id)\n",
    "        if doc_word_str in doc_word_freq:\n",
    "            doc_word_freq[doc_word_str] += 1\n",
    "        else:\n",
    "            doc_word_freq[doc_word_str] = 1\n",
    "\n",
    "for i in range(len(shuffle_doc_words_list)):\n",
    "    doc_words = shuffle_doc_words_list[i]\n",
    "    words = tokenizer.tokenize(doc_words)\n",
    "    doc_word_set = set()\n",
    "    for word in words:\n",
    "        if word in doc_word_set:\n",
    "            continue\n",
    "        j = word_id_map[word]\n",
    "        key = str(i) + ',' + str(j)\n",
    "        freq = doc_word_freq[key]\n",
    "        if i < train_size:\n",
    "            row.append(i)\n",
    "        else:\n",
    "            row.append(i + vocab_size)\n",
    "        col.append(train_size + j)\n",
    "        idf = log(1.0 * len(shuffle_doc_words_list) /\n",
    "                  word_doc_freq[vocab[j]])\n",
    "        weight.append(freq * idf)\n",
    "        doc_word_set.add(word)\n",
    "\n",
    "node_size = train_size + vocab_size + test_size\n",
    "adj = sp.csr_matrix(\n",
    "    (weight, (row, col)), shape=(node_size, node_size))\n",
    "\n",
    "f = open(\"data/ind.{}.x\".format(dataset), 'wb')\n",
    "pkl.dump(x, f)\n",
    "f.close()\n",
    "\n",
    "f = open(\"data/ind.{}.y\".format(dataset), 'wb')\n",
    "pkl.dump(y, f)\n",
    "f.close()\n",
    "\n",
    "f = open(\"data/ind.{}.test_x\".format(dataset), 'wb')\n",
    "pkl.dump(tx, f)\n",
    "f.close()\n",
    "\n",
    "f = open(\"data/ind.{}.test_y\".format(dataset), 'wb')\n",
    "pkl.dump(ty, f)\n",
    "f.close()\n",
    "\n",
    "f = open(\"data/ind.{}.all_x\".format(dataset), 'wb')\n",
    "pkl.dump(allx, f)\n",
    "f.close()\n",
    "\n",
    "f = open(\"data/ind.{}.all_y\".format(dataset), 'wb')\n",
    "pkl.dump(ally, f)\n",
    "f.close()\n",
    "\n",
    "f = open(\"data/ind.{}.adj\".format(dataset), 'wb')\n",
    "pkl.dump(adj, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5cf8754-0620-4acc-bb67-45bf2a08cf5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import networkx as nx\n",
    "import scipy.sparse as sp\n",
    "\n",
    "\n",
    "def parse_index_file(filename):\n",
    "    index = []\n",
    "    for line in open(filename):\n",
    "        index.append(int(line.strip()))\n",
    "    return index\n",
    "\n",
    "def load_data(dataset_str):\n",
    "    names = ['x', 'y', 'test_x', 'test_y', 'all_x', 'all_y', 'graph']\n",
    "    \n",
    "    objects = []\n",
    "    for i in range(len(names)):\n",
    "        with open(\"data/ind.{}.{}\".format(dataset_str, names[i]), 'rb') as f:\n",
    "            objects.append(pkl.load(f))\n",
    "\n",
    "    x, y, test_x, test_y, all_x, all_y, graph = tuple(objects)\n",
    "    test_idx_reorder = parse_index_file(\n",
    "        \"data/ind.{}.test.index\".format(dataset_str))\n",
    "    test_idx_range = np.sort(test_idx_reorder)\n",
    "    print(x.shape, y.shape, test_x.shape, test_y.shape, all_x.shape, all_y.shape)\n",
    "\n",
    "    features = sp.vstack((all_x, test_x)).tolil()\n",
    "    features[test_idx_reorder, :] = features[test_idx_range, :]\n",
    "    adj = nx.adjacency_matrix(nx.from_dict_of_lists(graph))\n",
    "\n",
    "    labels = np.vstack((all_y, test_y))\n",
    "    labels[test_idx_reorder, :] = labels[test_idx_range, :]\n",
    "\n",
    "    idx_test = test_idx_range.tolist()\n",
    "    idx_train = range(len(y))\n",
    "    idx_val = range(len(y), len(y)+500)\n",
    "\n",
    "    train_mask = sample_mask(idx_train, labels.shape[0])\n",
    "    val_mask = sample_mask(idx_val, labels.shape[0])\n",
    "    test_mask = sample_mask(idx_test, labels.shape[0])\n",
    "\n",
    "    y_train, y_val, y_test = np.zeros(labels.shape), np.zeros(labels.shape), np.zeros(labels.shape)\n",
    "    \n",
    "    y_train[train_mask, :] = labels[train_mask, :]\n",
    "    y_val[val_mask, :] = labels[val_mask, :]\n",
    "    y_test[test_mask, :] = labels[test_mask, :]\n",
    "\n",
    "    return adj, features, y_train, y_val, y_test, train_mask, val_mask, test_mask\n",
    "\n",
    "\n",
    "def load_corpus(dataset_str):\n",
    "    names = ['x', 'y', 'test_x', 'test_y', 'all_x', 'all_y', 'adj']\n",
    "    \n",
    "    objects = []\n",
    "    for i in range(len(names)):\n",
    "        with open(\"data/ind.{}.{}\".format(dataset_str, names[i]), 'rb') as f:\n",
    "            objects.append(pkl.load(f))\n",
    "\n",
    "    x, y, test_x, test_y, all_x, all_y, adj = tuple(objects)\n",
    "    print(x.shape, y.shape, test_x.shape, test_y.shape, all_x.shape, all_y.shape)\n",
    "\n",
    "    features = sp.vstack((all_x, test_x)).tolil()\n",
    "    labels = np.vstack((all_y, test_y))\n",
    "    print(\"LABELS\",len(labels))\n",
    "\n",
    "    train_idx_orig = parse_index_file(\n",
    "        \"data/{}.train.index\".format(dataset_str))\n",
    "    train_size = len(train_idx_orig)\n",
    "\n",
    "    val_size = train_size - x.shape[0]\n",
    "    test_size = test_x.shape[0]\n",
    "\n",
    "    idx_train = range(len(y))\n",
    "    idx_val = range(len(y), len(y) + val_size)\n",
    "    idx_test = range(all_x.shape[0], all_x.shape[0] + test_size)\n",
    "\n",
    "    train_mask = sample_mask(idx_train, labels.shape[0])\n",
    "    val_mask = sample_mask(idx_val, labels.shape[0])\n",
    "    test_mask = sample_mask(idx_test, labels.shape[0])\n",
    "\n",
    "    y_train, y_val, y_test = np.zeros(labels.shape), np.zeros(labels.shape), np.zeros(labels.shape)\n",
    "    y_train[train_mask, :] = labels[train_mask, :]\n",
    "    y_val[val_mask, :] = labels[val_mask, :]\n",
    "    y_test[test_mask, :] = labels[test_mask, :]\n",
    "\n",
    "    adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
    "    \n",
    "    return adj, features, y_train, y_val, y_test, train_mask, val_mask, test_mask, train_size, test_size\n",
    "\n",
    "def sparse_to_tuple(sparse_mx):\n",
    "    def to_tuple(mx):\n",
    "        if not sp.isspmatrix_coo(mx):\n",
    "            mx = mx.tocoo()\n",
    "        coords = np.vstack((mx.row, mx.col)).transpose()\n",
    "        values = mx.data\n",
    "        shape = mx.shape\n",
    "        return coords, values, shape\n",
    "\n",
    "    if isinstance(sparse_mx, list):\n",
    "        for i in range(len(sparse_mx)):\n",
    "            sparse_mx[i] = to_tuple(sparse_mx[i])\n",
    "    else:\n",
    "        sparse_mx = to_tuple(sparse_mx)\n",
    "\n",
    "    return sparse_mx\n",
    "\n",
    "\n",
    "def preprocess_features(features):\n",
    "    rowsum = np.array(features.sum(1))\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    features = r_mat_inv.dot(features)\n",
    "    return sparse_to_tuple(features)\n",
    "\n",
    "def sample_mask(idx, l):\n",
    "    mask = np.zeros(l)\n",
    "    mask[idx] = 1\n",
    "    return np.array(mask, dtype=np.bool)\n",
    "\n",
    "def preprocess_adj(adj):\n",
    "    adj_normalized = normalize_adj(adj + sp.eye(adj.shape[0]))\n",
    "    return sparse_to_tuple(adj_normalized)\n",
    "\n",
    "\n",
    "def normalize_adj(adj):\n",
    "    adj = sp.coo_matrix(adj)\n",
    "    rowsum = np.array(adj.sum(1))\n",
    "    d_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
    "    d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n",
    "    d_mat_inv_sqrt = sp.diags(d_inv_sqrt)\n",
    "    return adj.dot(d_mat_inv_sqrt).transpose().dot(d_mat_inv_sqrt).tocoo()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858f8d7a-aa2b-4f85-b131-9b45dee460bb",
   "metadata": {},
   "source": [
    "# GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "fed39e50-365b-4fa9-8096-cdeab70e5a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.utils.data as Data\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from utils import *\n",
    "import dgl\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "import argparse\n",
    "import sys\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from ignite.engine import Events, create_supervised_evaluator, create_supervised_trainer, Engine\n",
    "from ignite.metrics import Accuracy, Loss, Metric\n",
    "from ignite.contrib.handlers.tqdm_logger import ProgressBar\n",
    "from ignite.utils import manual_seed\n",
    "# from model import BertGCN\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModel, AutoTokenizer,BertTokenizerFast\n",
    "from dgl.nn.pytorch import GraphConv\n",
    "from dgl import function as fn\n",
    "from dgl.base import DGLError\n",
    "from dgl.utils import expand_as_pair\n",
    "\n",
    "class BertClassifier(torch.nn.Module):\n",
    "    def __init__(self, pretrained_model:str =\"roberta-base\", nb_class:int = 8):\n",
    "        super(BertClassifier, self).__init__()\n",
    "        self.nb_class = nb_class\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained('roberta-base')\n",
    "        self.bert_model = AutoModel.from_pretrained(pretrained_model)\n",
    "        self.feat_dim = list(self.bert_model.modules())[-2].out_features\n",
    "        self.classifier = torch.nn.Linear(self.feat_dim, nb_class)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        cls_feats = self.bert_model(input_ids, attention_mask)[0][:, 0]\n",
    "        cls_logit = self.classifier(cls_feats)\n",
    "        return cls_logit\n",
    "\n",
    "\n",
    "class BertGCN(torch.nn.Module):\n",
    "    def __init__(self, pretrained_model:str =\"roberta-base\", nb_class:int = 8, ratio:float = 0.7, gcn_layers:int = 2, n_hidden:int = 200, dropout:int = 0.5):\n",
    "        super(BertGCN, self).__init__()\n",
    "        self.ratio = ratio\n",
    "        self.nb_class = nb_class\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(pretrained_model)\n",
    "        self.bert_model = AutoModel.from_pretrained(pretrained_model)\n",
    "        \n",
    "        # segBERT_array = list(self.bert_model.modules())[-2].out_features\n",
    "        self.feat_dim = list(self.bert_model.modules())[-2].out_features\n",
    "        self.classifier = torch.nn.Linear(self.feat_dim, nb_class)\n",
    "        self.gcn = GCN(\n",
    "            in_feats=self.feat_dim,\n",
    "            n_hidden=n_hidden,\n",
    "            n_classes=nb_class,\n",
    "            n_layers=gcn_layers-1,\n",
    "            activation=F.elu,\n",
    "            dropout=dropout\n",
    "        )\n",
    "\n",
    "    def forward(self, g, idx):\n",
    "        # input_ids, attention_mask = g.ndata['input_ids'][idx], g.ndata['attention_mask'][idx]\n",
    "        # if self.training:\n",
    "        #     cls_feats = self.bert_model(input_ids, attention_mask)[0][:, 0]\n",
    "        #     g.ndata['cls_feats'][idx] = cls_feats\n",
    "        # else:\n",
    "        cls_feats = g.ndata['cls_feats'][idx]\n",
    "        cls_logit = self.classifier(cls_feats)\n",
    "        cls_pred = torch.nn.Softmax(dim=1)(cls_logit)\n",
    "        gcn_logit = self.gcn(g.ndata['cls_feats'], g, g.edata['edge_weight'])[idx]\n",
    "        gcn_pred = torch.nn.Softmax(dim=1)(gcn_logit)\n",
    "        pred = (gcn_pred+1e-10) * self.ratio + cls_pred * (1 - self.ratio)\n",
    "        pred = torch.log(pred)\n",
    "        return pred\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_feats,\n",
    "                 n_hidden,\n",
    "                 n_classes,\n",
    "                 n_layers,\n",
    "                 activation,\n",
    "                 dropout,\n",
    "                 normalization='none'):\n",
    "        super(GCN, self).__init__()\n",
    "        self.layers = torch.nn.ModuleList()\n",
    "        self.layers.append(GraphConvEdgeWeight(in_feats, n_hidden, activation=activation, norm=normalization))\n",
    "        for i in range(n_layers - 1):\n",
    "            self.layers.append(GraphConvEdgeWeight(n_hidden, n_hidden, activation=activation, norm=normalization))\n",
    "        self.layers.append(GraphConvEdgeWeight(n_hidden, n_classes, norm=normalization))\n",
    "        self.dropout = torch.nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, features, g, edge_weight):\n",
    "        h = features\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            if i != 0:\n",
    "                h = self.dropout(h)\n",
    "            h = layer(g, h, edge_weights=edge_weight)\n",
    "        return h\n",
    "    \n",
    "class GraphConvEdgeWeight(GraphConv):\n",
    "    def forward(self, graph, feat,  weight=None, edge_weights=None):\n",
    "        with graph.local_scope():\n",
    "            if not self._allow_zero_in_degree:\n",
    "                if (graph.in_degrees() == 0).any():\n",
    "                    raise DGLError('There are 0-in-degree nodes in the graph, '\n",
    "                                   'output for those nodes will be invalid. '\n",
    "                                   'This is harmful for some applications, '\n",
    "                                   'causing silent performance regression. '\n",
    "                                   'Adding self-loop on the input graph by '\n",
    "                                   'calling `g = dgl.add_self_loop(g)` will resolve '\n",
    "                                   'the issue. Setting ``allow_zero_in_degree`` '\n",
    "                                   'to be `True` when constructing this module will '\n",
    "                                   'suppress the check and let the code run.')\n",
    "\n",
    "            feat_src, feat_dst = expand_as_pair(feat, graph)\n",
    "            if self._norm == 'both':\n",
    "                degs = graph.out_degrees().float().clamp(min=1)\n",
    "                norm = torch.pow(degs, -0.5)\n",
    "                shp = norm.shape + (1,) * (feat_src.dim() - 1)\n",
    "                norm = torch.reshape(norm, shp)\n",
    "                feat_src = feat_src * norm\n",
    "\n",
    "            if weight is not None:\n",
    "                if self.weight is not None:\n",
    "                    raise DGLError('External weight is provided while at the same time the'\n",
    "                                   ' module has defined its own weight parameter. Please'\n",
    "                                   ' create the module with flag weight=False.')\n",
    "            else:\n",
    "                weight = self.weight\n",
    "\n",
    "            if self._in_feats > self._out_feats:\n",
    "                if weight is not None:\n",
    "                    feat_src = torch.matmul(feat_src, weight)\n",
    "                graph.srcdata['h'] = feat_src\n",
    "                if edge_weights is None:\n",
    "                    graph.update_all(fn.copy_src(src='h', out='m'),\n",
    "                                     fn.sum(msg='m', out='h'))\n",
    "                else:\n",
    "                    graph.edata['a'] = edge_weights\n",
    "                    graph.update_all(fn.u_mul_e('h', 'a', 'm'),\n",
    "                                     fn.sum(msg='m', out='h'))\n",
    "                rst = graph.dstdata['h']\n",
    "            else:\n",
    "                graph.srcdata['h'] = feat_src\n",
    "                if edge_weights is None:\n",
    "                    graph.update_all(fn.copy_src(src='h', out='m'),\n",
    "                                     fn.sum(msg='m', out='h'))\n",
    "                else:\n",
    "                    graph.edata['a'] = edge_weights\n",
    "                    graph.update_all(fn.u_mul_e('h', 'a', 'm'),\n",
    "                                     fn.sum(msg='m', out='h'))\n",
    "                rst = graph.dstdata['h']\n",
    "                if weight is not None:\n",
    "                    rst = torch.matmul(rst, weight)\n",
    "\n",
    "            if self._norm != 'none':\n",
    "                degs = graph.in_degrees().float().clamp(min=1)\n",
    "                if self._norm == 'both':\n",
    "                    norm = torch.pow(degs, -0.5)\n",
    "                else:\n",
    "                    norm = 1.0 / degs\n",
    "                shp = norm.shape + (1,) * (feat_dst.dim() - 1)\n",
    "                norm = torch.reshape(norm, shp)\n",
    "                rst = rst * norm\n",
    "\n",
    "            if self.bias is not None:\n",
    "                rst = rst + self.bias\n",
    "\n",
    "            if self._activation is not None:\n",
    "                rst = self._activation(rst)\n",
    "\n",
    "            return rst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "875ed1ac-0b35-402a-9032-8181435cafe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 51\n",
    "manual_seed(seed)\n",
    "max_length = 200\n",
    "batch_size = 32\n",
    "ratio = 0.1\n",
    "nb_epochs = 20\n",
    "dataset = \"book\"\n",
    "n_hidden = 200\n",
    "dropout = 0.5\n",
    "gcn_lr = 1e-3\n",
    "bert_lr = 1e-5\n",
    "args = [max_length, batch_size, ratio, nb_epochs, dataset, n_hidden, dropout, gcn_lr, bert_lr, seed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "ef0ddd44-66f6-4b42-8ca1-934b8dea04a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params:\n",
      "params:\n",
      "params:\n",
      "[200, 32, 0.1, 20, 'imdb', 200, 0.5, 0.001, 1e-05, 51]\n",
      "[200, 32, 0.1, 20, 'imdb', 200, 0.5, 0.001, 1e-05, 51]\n",
      "[200, 32, 0.1, 20, 'imdb', 200, 0.5, 0.001, 1e-05, 51]\n",
      "checkpoints path: ./checkpoint/robertagcn_imdb_0.1\n",
      "checkpoints path: ./checkpoint/robertagcn_imdb_0.1\n",
      "checkpoints path: ./checkpoint/robertagcn_imdb_0.1\n"
     ]
    }
   ],
   "source": [
    "ckpt_dir = './checkpoint/robertagcn_{}_{}'.format(dataset, ratio)\n",
    "os.makedirs(ckpt_dir, exist_ok=True)\n",
    "\n",
    "streamhandle = logging.StreamHandler(sys.stdout)\n",
    "streamhandle.setFormatter(logging.Formatter('%(message)s'))\n",
    "streamhandle.setLevel(logging.INFO)\n",
    "\n",
    "filehandle = logging.FileHandler(filename=os.path.join(ckpt_dir, 'training.log'), mode='w')\n",
    "filehandle.setFormatter(logging.Formatter('%(message)s'))\n",
    "filehandle.setLevel(logging.INFO)\n",
    "\n",
    "logger = logging.getLogger('training logger')\n",
    "logger.addHandler(streamhandle)\n",
    "logger.addHandler(filehandle)\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "cpu = torch.device('cpu')\n",
    "gpu = torch.device('cuda:0')\n",
    "\n",
    "logger.info('params:')\n",
    "logger.info(str(args))\n",
    "logger.info('checkpoints path: {}'.format(ckpt_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "34d939a4-aba7-4ab0-9aa1-6df1082fa412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1350, 300) (1350, 2) (2511, 300) (2511, 2) (3545, 300) (3545, 2)\n",
      "LABELS 6056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_817/2624410907.py:119: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  return np.array(mask, dtype=np.bool)\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph information:\n",
      "graph information:\n",
      "graph information:\n",
      "Graph(num_nodes=6056, num_edges=3981402,\n",
      "      ndata_schemes={'label': Scheme(shape=(), dtype=torch.int64), 'train': Scheme(shape=(), dtype=torch.float32), 'val': Scheme(shape=(), dtype=torch.float32), 'test': Scheme(shape=(), dtype=torch.float32), 'label_train': Scheme(shape=(), dtype=torch.int64), 'cls_feats': Scheme(shape=(768,), dtype=torch.float32)}\n",
      "      edata_schemes={'edge_weight': Scheme(shape=(), dtype=torch.float32)})\n",
      "Graph(num_nodes=6056, num_edges=3981402,\n",
      "      ndata_schemes={'label': Scheme(shape=(), dtype=torch.int64), 'train': Scheme(shape=(), dtype=torch.float32), 'val': Scheme(shape=(), dtype=torch.float32), 'test': Scheme(shape=(), dtype=torch.float32), 'label_train': Scheme(shape=(), dtype=torch.int64), 'cls_feats': Scheme(shape=(768,), dtype=torch.float32)}\n",
      "      edata_schemes={'edge_weight': Scheme(shape=(), dtype=torch.float32)})\n",
      "Graph(num_nodes=6056, num_edges=3981402,\n",
      "      ndata_schemes={'label': Scheme(shape=(), dtype=torch.int64), 'train': Scheme(shape=(), dtype=torch.float32), 'val': Scheme(shape=(), dtype=torch.float32), 'test': Scheme(shape=(), dtype=torch.float32), 'label_train': Scheme(shape=(), dtype=torch.int64), 'cls_feats': Scheme(shape=(768,), dtype=torch.float32)}\n",
      "      edata_schemes={'edge_weight': Scheme(shape=(), dtype=torch.float32)})\n"
     ]
    }
   ],
   "source": [
    "adj, features, y_train, y_val, y_test, train_mask, val_mask, test_mask, train_size, test_size = load_corpus(dataset)\n",
    "\n",
    "nb_node = features.shape[0]\n",
    "nb_train, nb_val, nb_test = train_mask.sum(), val_mask.sum(), test_mask.sum()\n",
    "nb_word = nb_node - nb_train - nb_val - nb_test\n",
    "nb_class = y_train.shape[1]\n",
    "\n",
    "\n",
    "model = BertGCN(nb_class=nb_class, ratio=ratio, n_hidden=n_hidden, dropout=dropout)\n",
    "\n",
    "corpse_file = './data/corpus/' + dataset +'_shuffle.txt'\n",
    "with open(corpse_file, 'r', encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "    text = text.replace('\\\\', '')\n",
    "    text = text.split('\\n')\n",
    "\n",
    "y = y_train + y_test + y_val\n",
    "y_train = y_train.argmax(axis=1)\n",
    "y = y.argmax(axis=1)\n",
    "\n",
    "doc_mask  = train_mask + val_mask + test_mask\n",
    "\n",
    "adj_norm = normalize_adj(adj + sp.eye(adj.shape[0]))\n",
    "g = dgl.from_scipy(adj_norm.astype('float32'), eweight_name='edge_weight')\n",
    "# g.ndata['input_ids'], g.ndata['attention_mask'] = input_ids, attention_mask\n",
    "g.ndata['label'], g.ndata['train'], g.ndata['val'], g.ndata['test'] = \\\n",
    "    torch.LongTensor(y), torch.FloatTensor(train_mask), torch.FloatTensor(val_mask), torch.FloatTensor(test_mask)\n",
    "g.ndata['label_train'] = torch.LongTensor(y_train)\n",
    "g.ndata['cls_feats'] = torch.zeros((nb_node, model.feat_dim))\n",
    "\n",
    "logger.info('graph information:')\n",
    "logger.info(str(g))\n",
    "\n",
    "train_idx = Data.TensorDataset(torch.arange(0, nb_train, dtype=torch.long))\n",
    "val_idx = Data.TensorDataset(torch.arange(nb_train, nb_train + nb_val, dtype=torch.long))\n",
    "test_idx = Data.TensorDataset(torch.arange(nb_node-nb_test, nb_node, dtype=torch.long))\n",
    "doc_idx = Data.ConcatDataset([train_idx, val_idx, test_idx])\n",
    "\n",
    "idx_loader_train = Data.DataLoader(train_idx, batch_size=batch_size)\n",
    "idx_loader_val = Data.DataLoader(val_idx, batch_size=batch_size)\n",
    "idx_loader_test = Data.DataLoader(test_idx, batch_size=batch_size)\n",
    "idx_loader = Data.DataLoader(doc_idx, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "a7d52d5f-8eb3-4a5e-8f28-e6798d6e18f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class F1Score(Metric):\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self.f1 = 0\n",
    "        self.count = 0\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def update(self, output):\n",
    "        y_pred, y = output[0].detach(), output[1].detach()\n",
    "\n",
    "        _, predicted = torch.max(y_pred, 1)\n",
    "        f = f1_score(y.cpu(), predicted.cpu(), average='micro')\n",
    "        self.f1 += f\n",
    "        self.count += 1\n",
    "\n",
    "    def reset(self):\n",
    "        self.f1 = 0\n",
    "        self.count = 0\n",
    "        super(F1Score, self).reset()\n",
    "\n",
    "    def compute(self):\n",
    "        return self.f1 / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "549351ae-39a1-48a7-8824-ea616b2f34fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################\n",
    "#\n",
    "# Custom_Dataset_Class.py\n",
    "# This file contains the code to load and prepare the dataset\n",
    "# for use by BERT.\n",
    "# It does preprocessing, segmentation and BERT features extraction\n",
    "#\n",
    "##############################################################\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "import os \n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import transformers\n",
    "# get_linear_schedule_with_warmup\n",
    "from transformers import RobertaTokenizer, BertTokenizer, RobertaModel, BertModel, AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import time\n",
    "\n",
    "\n",
    "class ConsumerComplaintsDataset1(Dataset):\n",
    "    \"\"\" Make preprocecing, tokenization and transform consumer complaints\n",
    "    dataset into pytorch DataLoader instance.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    tokenizer: BertTokenizer\n",
    "        transform data into feature that bert understand\n",
    "    max_len: int\n",
    "        the max number of token in a sequence in bert tokenization. \n",
    "    overlap_len: int\n",
    "        the maximum number of overlap token.\n",
    "    chunk_len: int\n",
    "        define the maximum number of word in a single chunk when spliting sample into a chumk\n",
    "    approach: str\n",
    "        define how to handle overlap token after bert tokenization.\n",
    "    max_size_dataset: int\n",
    "        define the maximum number of sample to used from data.\n",
    "    file_location: str\n",
    "        the path of the dataset.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    data: array of shape (n_keept_sample,)\n",
    "        prepocess data.\n",
    "    label: array of shape (n_keept_sample,)\n",
    "        data labels\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, tokenizer, max_len, chunk_len=200, overlap_len=50, approach=\"all\", max_size_dataset=None, file_location=\"../dataset/IMDB/IMDB_over512.csv\", min_len=512):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.overlap_len = overlap_len\n",
    "        self.chunk_len = chunk_len\n",
    "        self.approach = approach\n",
    "        self.min_len = min_len\n",
    "        self.max_size_dataset = max_size_dataset\n",
    "        self.data, self.label = self.process_data(file_location,)\n",
    "\n",
    "    def process_data(self, file_location):\n",
    "        \"\"\" Preprocess the text and label columns as describe in the paper.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        file_location: str\n",
    "            the path of the dataset file.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        texts: array of shape (n_keept_sample,)\n",
    "            preprocessed  sample\n",
    "        labels: array (n_keept_sample,)\n",
    "            samples labels transform into a numerical value\n",
    "        \"\"\"\n",
    "        # Load the dataset into a pandas dataframe.\n",
    "        # df = pd.read_csv(file_location, dtype=\"unicode\")\n",
    "        # return train['text'].values, train['label'].values\n",
    "        df= pd.read_csv(\"../dataset/IMDB/IMDB_over512.csv\", names = [\"label\", \"text\"])\n",
    "        LE = LabelEncoder()\n",
    "        df['label'] = LE.fit_transform(df['label'])\n",
    "        train = df.copy()\n",
    "        if(self.max_size_dataset):\n",
    "            train = train.loc[0:self.max_size_dataset, :]\n",
    "        # train = train.reindex(np.random.permutation(train.index))\n",
    "        train['text'] = train.text.apply(self.clean_txt)\n",
    "        return train['text'].values, train['label'].values\n",
    "\n",
    "\n",
    "\n",
    "    def clean_txt(self, text):\n",
    "        \"\"\" Remove special characters from text \"\"\"\n",
    "        text = re.sub(\"'\", \"\", text)\n",
    "        text = re.sub(\"(\\\\W)+\", \" \", text)\n",
    "        return text\n",
    "\n",
    "    def long_terms_tokenizer(self, data_tokenize, targets):\n",
    "        \"\"\"  tranfrom tokenized data into a long token that take care of\n",
    "        overflow token according to the specified approch.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        data_tokenize: dict\n",
    "            an tokenized result of a sample from bert tokenizer encode_plus method.\n",
    "        targets: array\n",
    "            labels of each samples.\n",
    "\n",
    "        Returns\n",
    "        _______\n",
    "        long_token: dict\n",
    "            a dictionnary that contains\n",
    "             - [ids]  tokens ids\n",
    "             - [mask] attention mask of each token\n",
    "             - [token_types_ids] the type ids of each token. note that each token in the same sequence has the same type ids\n",
    "             - [targets_list] list of all sample label after add overlap token as sample according to the approach used\n",
    "             - [len] length of targets_list\n",
    "        \"\"\"\n",
    "\n",
    "        long_terms_token = []\n",
    "        input_ids_list = []\n",
    "        attention_mask_list = []\n",
    "        token_type_ids_list = []\n",
    "        targets_list = []\n",
    "        count = 0\n",
    "        previous_input_ids = data_tokenize[\"input_ids\"].reshape(-1)\n",
    "        previous_attention_mask = data_tokenize[\"attention_mask\"].reshape(-1)\n",
    "        previous_token_type_ids = data_tokenize[\"token_type_ids\"].reshape(-1)\n",
    "        remain = data_tokenize.get(\"overflowing_tokens\").view(-1)\n",
    "        targets = torch.tensor(targets, dtype=torch.int)\n",
    "        input_ids_list.append(previous_input_ids)\n",
    "        attention_mask_list.append(previous_attention_mask)\n",
    "        token_type_ids_list.append(previous_token_type_ids)\n",
    "        targets_list.append(targets)\n",
    "        if remain is not None and self.approach != 'head':\n",
    "            remain = torch.tensor(remain, dtype=torch.long)\n",
    "            idxs = range(len(remain)+self.chunk_len)\n",
    "            idxs = idxs[(self.chunk_len-self.overlap_len-2)\n",
    "                         ::(self.chunk_len-self.overlap_len-2)]\n",
    "            input_ids_first_overlap = previous_input_ids[-(\n",
    "                self.overlap_len+1):-1]\n",
    "           \n",
    "            start_token = torch.tensor([101], dtype=torch.long)\n",
    "            end_token = torch.tensor([102], dtype=torch.long)\n",
    "\n",
    "            for i, idx in enumerate(idxs):\n",
    "                if i == 0:\n",
    "                    input_ids = torch.cat( \n",
    "                        (input_ids_first_overlap, remain[:idx]))\n",
    "                elif i == len(idxs):\n",
    "                    input_ids = remain[idx:].view(-1)\n",
    "                elif previous_idx >= len(remain):\n",
    "                    break\n",
    "                else:\n",
    "                    input_ids = remain[(previous_idx-self.overlap_len):idx]\n",
    "\n",
    "                previous_idx = idx\n",
    "\n",
    "                nb_token = len(input_ids)+2\n",
    "                attention_mask = torch.ones(self.chunk_len, dtype=torch.long)\n",
    "                attention_mask[nb_token:self.chunk_len] = 0\n",
    "                token_type_ids = torch.zeros(self.chunk_len, dtype=torch.long)\n",
    "                input_ids = torch.cat((start_token, input_ids, end_token))\n",
    "                if self.chunk_len-nb_token > 0:\n",
    "                    padding = torch.zeros(\n",
    "                        self.chunk_len-nb_token, dtype=torch.long)\n",
    "                    input_ids = torch.cat((input_ids, padding))\n",
    "\n",
    "                input_ids_list.append(input_ids)\n",
    "                attention_mask_list.append(attention_mask)\n",
    "                token_type_ids_list.append(token_type_ids)\n",
    "                targets_list.append(targets)\n",
    "                count = count +1 \n",
    "            if self.approach == \"tail\":\n",
    "                input_ids_list = [input_ids_list[-1]]\n",
    "                attention_mask_list = [attention_mask_list[-1]]\n",
    "                token_type_ids_list = [token_type_ids_list[-1]]\n",
    "                targets_list = [targets_list[-1]]\n",
    "             \n",
    "        # print(count)\n",
    "        return({\n",
    "            'ids': input_ids_list,  # torch.tensor(ids, dtype=torch.long),\n",
    "            # torch.tensor(mask, dtype=torch.long),\n",
    "            'mask': attention_mask_list,\n",
    "            # torch.tensor(token_type_ids, dtype=torch.long),\n",
    "            'token_type_ids': token_type_ids_list,\n",
    "            'targets': targets_list,\n",
    "            'len': [torch.tensor(len(targets_list), dtype=torch.long)]\n",
    "        })\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"  Return a single tokenized sample at a given positon [idx] from data\"\"\"\n",
    "        consumer_complaint = str(self.data[idx])\n",
    "        targets = int(self.label[idx])\n",
    "        data = self.tokenizer.encode_plus(\n",
    "            consumer_complaint,\n",
    "            max_length=200,\n",
    "            pad_to_max_length=True,\n",
    "            add_special_tokens=True,\n",
    "            return_attention_mask=True,\n",
    "            return_token_type_ids=True,\n",
    "            return_overflowing_tokens=True,\n",
    "            return_tensors='pt')\n",
    "\n",
    "        long_token = self.long_terms_tokenizer(data, targets)\n",
    "        return long_token\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\" Return data length \"\"\"\n",
    "        return self.label.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "b22b68e2-4afa-4e94-9066-7ac8fc744a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np \n",
    "import torch \n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(1)\n",
    "# attention layer code inspired from: https://discuss.pytorch.org/t/self-attention-on-words-and-masking/5671/4\n",
    "\n",
    "import torch.nn.functional as F\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_size, batch_first=False):\n",
    "        super(Attention, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.batch_first = batch_first\n",
    "\n",
    "        self.att_weights = nn.Parameter(torch.Tensor(1, hidden_size), requires_grad=True)\n",
    "\n",
    "        stdv = 1.0 / np.sqrt(self.hidden_size)\n",
    "        for weight in self.att_weights:\n",
    "            nn.init.uniform_(weight, -stdv, stdv)\n",
    "\n",
    "    def get_mask(self):\n",
    "        pass\n",
    "\n",
    "    def forward(self, inputs, lengths):\n",
    "        if self.batch_first:\n",
    "            batch_size, max_len = inputs.size()[:2]\n",
    "        else:\n",
    "            max_len, batch_size = inputs.size()[:2]\n",
    "            \n",
    "        # apply attention layer\n",
    "        weights = torch.bmm(inputs,\n",
    "                            self.att_weights  # (1, hidden_size)\n",
    "                            .permute(1, 0)  # (hidden_size, 1)\n",
    "                            .unsqueeze(0)  # (1, hidden_size, 1)\n",
    "                            .repeat(batch_size, 1, 1) # (batch_size, hidden_size, 1)\n",
    "                            )\n",
    "    \n",
    "        attentions = torch.softmax(F.relu(weights.squeeze()), dim=-1)\n",
    "\n",
    "        # create mask based on the sentence lengths\n",
    "        mask = torch.ones(attentions.size(), requires_grad=True).cuda()\n",
    "        for i, l in enumerate(lengths):  # skip the first sentence\n",
    "            if l < max_len:\n",
    "                mask[i, l:] = 0\n",
    "\n",
    "        # apply mask and renormalize attention scores (weights)\n",
    "        masked = attentions * mask\n",
    "        _sums = masked.sum(-1).unsqueeze(-1)  # sums per row\n",
    "        \n",
    "        attentions = masked.div(_sums)\n",
    "\n",
    "        # apply attention weights\n",
    "        weighted = torch.mul(inputs, attentions.unsqueeze(-1).expand_as(inputs))\n",
    "\n",
    "        # get the final fixed vector representations of the sentences\n",
    "        representations = weighted.sum(1).squeeze()\n",
    "\n",
    "        return representations, attentions\n",
    "    \n",
    "    \n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import transformers\n",
    "# get_linear_schedule_with_warmup\n",
    "from transformers import RobertaTokenizer, BertTokenizer, RobertaModel, BertModel, AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import transformers\n",
    "# get_linear_schedule_with_warmup\n",
    "from transformers import RobertaTokenizer, BertTokenizer, RobertaModel, BertModel, AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import time\n",
    "\n",
    "\n",
    "# let the output dimension to be 768. \n",
    "class Bert_Attention1(nn.Module):\n",
    "    def __init__(self, bertFineTuned):\n",
    "        super(Bert_Attention1, self).__init__()\n",
    "        hidden_dim = 768\n",
    "        self.bertFineTuned = bertFineTuned\n",
    "        self.atten1 = Attention(hidden_dim,  batch_first=True) # 2 is bidrectional\n",
    "        self.fc1 = nn.Sequential(nn.Linear(768, 128),\n",
    "                                 nn.ReLU()) \n",
    "        self.out = nn.Linear(128, 768)\n",
    "\n",
    "\n",
    "    def forward(self, ids, mask, token_type_ids, lengt):\n",
    "        _, pooled_out = self.bertFineTuned(\n",
    "            ids, attention_mask=mask, token_type_ids=token_type_ids)\n",
    "        chunks_emb = pooled_out.split_with_sizes(lengt)\n",
    "        seq_lengths = torch.LongTensor([x for x in map(len, chunks_emb)])\n",
    "        batch_emb_pad = nn.utils.rnn.pad_sequence(\n",
    "            chunks_emb, padding_value=-91, batch_first=True)\n",
    "        batch_emb = batch_emb_pad.transpose(0, 1)  # (B,L,D) -> (L,B,D)\n",
    "        lstm_input = nn.utils.rnn.pack_padded_sequence(\n",
    "            batch_emb, seq_lengths.cpu().numpy(), batch_first=False, enforce_sorted=False)\n",
    "        x, lengths = nn.utils.rnn.pad_packed_sequence(lstm_input, batch_first=True)\n",
    "        x, _ = self.atten1(x, lengths)\n",
    "        x = x.view(-1, 768)\n",
    "        x = self.fc1(x)\n",
    "        z = self.out(x)\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "b80731d0-281a-4a98-8df9-4ff0564ed218",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT tokenizer...\n",
      "2005 (4011,)\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x7f431e4193f0>\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "TRAIN_BATCH_SIZE=8\n",
    "EPOCH=3\n",
    "validation_split = .5\n",
    "shuffle_dataset = True\n",
    "random_seed= 42\n",
    "MIN_LEN=512\n",
    "MAX_LEN = 100000\n",
    "CHUNK_LEN=200\n",
    "OVERLAP_LEN=50\n",
    "#MAX_LEN=10000000\n",
    "#MAX_SIZE_DATASET=1000\n",
    "\n",
    "print('Loading BERT tokenizer...')\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "dataset=ConsumerComplaintsDataset1(\n",
    "    tokenizer=bert_tokenizer,\n",
    "    min_len=MIN_LEN,\n",
    "    max_len=MAX_LEN,\n",
    "    chunk_len=CHUNK_LEN,\n",
    "    #max_size_dataset=MAX_SIZE_DATASET,\n",
    "    overlap_len=OVERLAP_LEN)\n",
    "\n",
    "\n",
    "# Creating data indices for training and validation splits:\n",
    "dataset_size = len(dataset)\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(validation_split * dataset_size))\n",
    "if shuffle_dataset :\n",
    "    np.random.seed(random_seed)\n",
    "    np.random.shuffle(indices)\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "# val_indices,train_indices  = indices[split:], indices[:split]\n",
    "\n",
    "print(split,np.shape(indices))\n",
    "# Creating PT data samplers and loaders:\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "valid_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "def my_collate1(batches):\n",
    "    return [{key: torch.stack(value) for key, value in batch.items()} for batch in batches]\n",
    "\n",
    "\n",
    "data_loader= DataLoader(\n",
    "    dataset,\n",
    "    batch_size=1,\n",
    "    collate_fn=my_collate1,\n",
    "    shuffle=False, \n",
    "    sampler=None\n",
    ")\n",
    "\n",
    "print(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "bf688943-6a8c-4776-ae9e-6cf230847c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "device = 'cuda'\n",
    "def update_feature():\n",
    "    global  g, doc_mask\n",
    "    model2 = torch.load(\"../other_model/imdb_attention_lstm_model_rnn_epoch3.pt\")\n",
    "    model2= Bert_Attention1(bertFineTuned=list(model2.children())[0]).to(device)\n",
    "    with torch.no_grad():\n",
    "        cls_list= []\n",
    "        for batch_idx, batch in enumerate(data_loader):\n",
    "            ids = [data[\"ids\"] for data in batch]\n",
    "            mask = [data[\"mask\"] for data in batch]\n",
    "            token_type_ids = [data[\"token_type_ids\"] for data in batch]\n",
    "            targets = [data[\"targets\"][0] for data in batch]\n",
    "            lengt = [data['len'] for data in batch]\n",
    "            ids = torch.cat(ids)\n",
    "            mask = torch.cat(mask)\n",
    "            token_type_ids = torch.cat(token_type_ids)\n",
    "            targets = torch.stack(targets)\n",
    "            lengt = torch.cat(lengt)\n",
    "            lengt = [x.item() for x in lengt]\n",
    "            ids = ids.to(device, dtype=torch.long)\n",
    "            mask = mask.to(device, dtype=torch.long)\n",
    "            token_type_ids = token_type_ids.to(device, dtype=torch.long)\n",
    "            targets = targets.to(device, dtype=torch.long)\n",
    "            out = model2(ids=ids, mask=mask,token_type_ids=token_type_ids, lengt=lengt)\n",
    "            cls_list.append(out.cpu())\n",
    "            # cls_feat =np.concatenate(out)\n",
    "        cls_feat = torch.cat(cls_list, axis=0)\n",
    "    # print(\"cls_feat\",np.shape(cls_feat))           \n",
    "    g = g.to(cpu)    \n",
    "    g.ndata['cls_feats'][doc_mask] = cls_feat\n",
    "    return g\n",
    "\n",
    "    \n",
    "\n",
    "optimizer = torch.optim.Adam([\n",
    "        {'params': model.bert_model.parameters(), 'lr': bert_lr},\n",
    "        {'params': model.classifier.parameters(), 'lr': bert_lr},\n",
    "        {'params': model.gcn.parameters(), 'lr': gcn_lr},\n",
    "    ], lr=gcn_lr\n",
    ")\n",
    "scheduler = lr_scheduler.MultiStepLR(optimizer, milestones=[30], gamma=0.1)\n",
    "\n",
    "\n",
    "def train_step(engine, batch):\n",
    "    global model, g, optimizer\n",
    "    model.train()\n",
    "    model = model.to(gpu)\n",
    "    g = g.to(gpu)\n",
    "    optimizer.zero_grad()\n",
    "    (idx, ) = [x.to(gpu) for x in batch]\n",
    "    optimizer.zero_grad()\n",
    "    train_mask = g.ndata['train'][idx].type(torch.BoolTensor)\n",
    "    y_pred = model(g, idx)[train_mask]\n",
    "    y_true = g.ndata['label_train'][idx][train_mask]\n",
    "    loss = F.nll_loss(y_pred, y_true)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    g.ndata['cls_feats'].detach_()\n",
    "    train_loss = loss.item()\n",
    "    with torch.no_grad():\n",
    "        if train_mask.sum() > 0:\n",
    "            y_true = y_true.detach().cpu()\n",
    "            y_pred = y_pred.argmax(axis=1).detach().cpu()\n",
    "            train_acc = accuracy_score(y_true, y_pred)\n",
    "        else:\n",
    "            train_acc = 1\n",
    "    return train_loss, train_acc\n",
    "\n",
    "\n",
    "trainer = Engine(train_step)\n",
    "pbar = ProgressBar()\n",
    "pbar.attach(trainer)\n",
    "\n",
    "@trainer.on(Events.EPOCH_COMPLETED)\n",
    "def reset_graph(trainer):\n",
    "    scheduler.step()\n",
    "    update_feature()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "def test_step(engine, batch):\n",
    "    global model, g\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        model = model.to(gpu)\n",
    "        g = g.to(gpu)\n",
    "        (idx, ) = [x.to(gpu) for x in batch]\n",
    "        y_pred = model(g, idx)\n",
    "        y_true = g.ndata['label'][idx]\n",
    "        return y_pred, y_true\n",
    "\n",
    "\n",
    "evaluator = Engine(test_step)\n",
    "eval_pbar = ProgressBar()\n",
    "eval_pbar.attach(evaluator)\n",
    "metrics={\n",
    "    'acc': Accuracy(),\n",
    "    'nll': Loss(torch.nn.NLLLoss()),\n",
    "    'f1' : F1Score()\n",
    "}\n",
    "for name, function in metrics.items():\n",
    "    function.attach(evaluator, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "29e745ca-508e-40d0-b4c6-5b043dfd09da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2346: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": "{desc}[{n_fmt}/{total_fmt}] {percentage:3.0f}%|{bar}{postfix} [{elapsed}<{remaining}]",
       "colour": null,
       "elapsed": 0.01165151596069336,
       "initial": 1,
       "n": 1,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 126,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[1/126]   1%|           [00:00<?]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2346: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": "{desc}[{n_fmt}/{total_fmt}] {percentage:3.0f}%|{bar}{postfix} [{elapsed}<{remaining}]",
       "colour": null,
       "elapsed": 0.011899471282958984,
       "initial": 1,
       "n": 1,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 43,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[1/43]   2%|2          [00:00<?]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": "{desc}[{n_fmt}/{total_fmt}] {percentage:3.0f}%|{bar}{postfix} [{elapsed}<{remaining}]",
       "colour": null,
       "elapsed": 0.00882577896118164,
       "initial": 1,
       "n": 1,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 5,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[1/5]  20%|##         [00:00<?]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": "{desc}[{n_fmt}/{total_fmt}] {percentage:3.0f}%|{bar}{postfix} [{elapsed}<{remaining}]",
       "colour": null,
       "elapsed": 0.008839130401611328,
       "initial": 1,
       "n": 1,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 79,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[1/79]   1%|1          [00:00<?]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1  Train acc: 0.5185 loss: 0.6148  Val acc: 0.5667 loss: 0.6011 f1: 0.5767  Test acc: 0.5504 loss: 0.6082 f1: 0.5485\n",
      "Epoch: 1  Train acc: 0.5185 loss: 0.6148  Val acc: 0.5667 loss: 0.6011 f1: 0.5767  Test acc: 0.5504 loss: 0.6082 f1: 0.5485\n",
      "Epoch: 1  Train acc: 0.5185 loss: 0.6148  Val acc: 0.5667 loss: 0.6011 f1: 0.5767  Test acc: 0.5504 loss: 0.6082 f1: 0.5485\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": "{desc}[{n_fmt}/{total_fmt}] {percentage:3.0f}%|{bar}{postfix} [{elapsed}<{remaining}]",
       "colour": null,
       "elapsed": 0.011081457138061523,
       "initial": 1,
       "n": 1,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 126,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[1/126]   1%|           [00:00<?]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2346: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": "{desc}[{n_fmt}/{total_fmt}] {percentage:3.0f}%|{bar}{postfix} [{elapsed}<{remaining}]",
       "colour": null,
       "elapsed": 0.011658430099487305,
       "initial": 1,
       "n": 1,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 43,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[1/43]   2%|2          [00:00<?]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": "{desc}[{n_fmt}/{total_fmt}] {percentage:3.0f}%|{bar}{postfix} [{elapsed}<{remaining}]",
       "colour": null,
       "elapsed": 0.010544776916503906,
       "initial": 1,
       "n": 1,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 5,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[1/5]  20%|##         [00:00<?]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": "{desc}[{n_fmt}/{total_fmt}] {percentage:3.0f}%|{bar}{postfix} [{elapsed}<{remaining}]",
       "colour": null,
       "elapsed": 0.010533332824707031,
       "initial": 1,
       "n": 1,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 79,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[1/79]   1%|1          [00:00<?]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2  Train acc: 0.9563 loss: 0.6147  Val acc: 0.9867 loss: 0.6064 f1: 0.9875  Test acc: 0.9626 loss: 0.6115 f1: 0.9624\n",
      "Epoch: 2  Train acc: 0.9563 loss: 0.6147  Val acc: 0.9867 loss: 0.6064 f1: 0.9875  Test acc: 0.9626 loss: 0.6115 f1: 0.9624\n",
      "Epoch: 2  Train acc: 0.9563 loss: 0.6147  Val acc: 0.9867 loss: 0.6064 f1: 0.9875  Test acc: 0.9626 loss: 0.6115 f1: 0.9624\n",
      "New checkpoint\n",
      "New checkpoint\n",
      "New checkpoint\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": "{desc}[{n_fmt}/{total_fmt}] {percentage:3.0f}%|{bar}{postfix} [{elapsed}<{remaining}]",
       "colour": null,
       "elapsed": 0.01104593276977539,
       "initial": 1,
       "n": 1,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 126,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[1/126]   1%|           [00:00<?]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2346: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": "{desc}[{n_fmt}/{total_fmt}] {percentage:3.0f}%|{bar}{postfix} [{elapsed}<{remaining}]",
       "colour": null,
       "elapsed": 0.012137889862060547,
       "initial": 1,
       "n": 1,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 43,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[1/43]   2%|2          [00:00<?]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": "{desc}[{n_fmt}/{total_fmt}] {percentage:3.0f}%|{bar}{postfix} [{elapsed}<{remaining}]",
       "colour": null,
       "elapsed": 0.00935816764831543,
       "initial": 1,
       "n": 1,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 5,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[1/5]  20%|##         [00:00<?]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": "{desc}[{n_fmt}/{total_fmt}] {percentage:3.0f}%|{bar}{postfix} [{elapsed}<{remaining}]",
       "colour": null,
       "elapsed": 0.008845806121826172,
       "initial": 1,
       "n": 1,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 79,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[1/79]   1%|1          [00:00<?]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3  Train acc: 0.5148 loss: 0.6670  Val acc: 0.5667 loss: 0.6533 f1: 0.5767  Test acc: 0.5472 loss: 0.6593 f1: 0.5449\n",
      "Epoch: 3  Train acc: 0.5148 loss: 0.6670  Val acc: 0.5667 loss: 0.6533 f1: 0.5767  Test acc: 0.5472 loss: 0.6593 f1: 0.5449\n",
      "Epoch: 3  Train acc: 0.5148 loss: 0.6670  Val acc: 0.5667 loss: 0.6533 f1: 0.5767  Test acc: 0.5472 loss: 0.6593 f1: 0.5449\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": "{desc}[{n_fmt}/{total_fmt}] {percentage:3.0f}%|{bar}{postfix} [{elapsed}<{remaining}]",
       "colour": null,
       "elapsed": 0.010967493057250977,
       "initial": 1,
       "n": 1,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 126,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[1/126]   1%|           [00:00<?]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2346: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": "{desc}[{n_fmt}/{total_fmt}] {percentage:3.0f}%|{bar}{postfix} [{elapsed}<{remaining}]",
       "colour": null,
       "elapsed": 0.011822938919067383,
       "initial": 1,
       "n": 1,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 43,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[1/43]   2%|2          [00:00<?]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": "{desc}[{n_fmt}/{total_fmt}] {percentage:3.0f}%|{bar}{postfix} [{elapsed}<{remaining}]",
       "colour": null,
       "elapsed": 0.010931253433227539,
       "initial": 1,
       "n": 1,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 5,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[1/5]  20%|##         [00:00<?]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": "{desc}[{n_fmt}/{total_fmt}] {percentage:3.0f}%|{bar}{postfix} [{elapsed}<{remaining}]",
       "colour": null,
       "elapsed": 0.012632369995117188,
       "initial": 1,
       "n": 1,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 79,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[1/79]   1%|1          [00:00<?]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4  Train acc: 0.5141 loss: 0.7307  Val acc: 0.5667 loss: 0.7264 f1: 0.5767  Test acc: 0.5472 loss: 0.7273 f1: 0.5449\n",
      "Epoch: 4  Train acc: 0.5141 loss: 0.7307  Val acc: 0.5667 loss: 0.7264 f1: 0.5767  Test acc: 0.5472 loss: 0.7273 f1: 0.5449\n",
      "Epoch: 4  Train acc: 0.5141 loss: 0.7307  Val acc: 0.5667 loss: 0.7264 f1: 0.5767  Test acc: 0.5472 loss: 0.7273 f1: 0.5449\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": "{desc}[{n_fmt}/{total_fmt}] {percentage:3.0f}%|{bar}{postfix} [{elapsed}<{remaining}]",
       "colour": null,
       "elapsed": 0.011004209518432617,
       "initial": 1,
       "n": 1,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 126,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[1/126]   1%|           [00:00<?]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2346: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": "{desc}[{n_fmt}/{total_fmt}] {percentage:3.0f}%|{bar}{postfix} [{elapsed}<{remaining}]",
       "colour": null,
       "elapsed": 0.011694908142089844,
       "initial": 1,
       "n": 1,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 43,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[1/43]   2%|2          [00:00<?]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": "{desc}[{n_fmt}/{total_fmt}] {percentage:3.0f}%|{bar}{postfix} [{elapsed}<{remaining}]",
       "colour": null,
       "elapsed": 0.010623455047607422,
       "initial": 1,
       "n": 1,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 5,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[1/5]  20%|##         [00:00<?]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": "{desc}[{n_fmt}/{total_fmt}] {percentage:3.0f}%|{bar}{postfix} [{elapsed}<{remaining}]",
       "colour": null,
       "elapsed": 0.010469198226928711,
       "initial": 1,
       "n": 1,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 79,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[1/79]   1%|1          [00:00<?]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5  Train acc: 0.4852 loss: 0.7384  Val acc: 0.4333 loss: 0.7522 f1: 0.4233  Test acc: 0.4528 loss: 0.7463 f1: 0.4551\n",
      "Epoch: 5  Train acc: 0.4852 loss: 0.7384  Val acc: 0.4333 loss: 0.7522 f1: 0.4233  Test acc: 0.4528 loss: 0.7463 f1: 0.4551\n",
      "Epoch: 5  Train acc: 0.4852 loss: 0.7384  Val acc: 0.4333 loss: 0.7522 f1: 0.4233  Test acc: 0.4528 loss: 0.7463 f1: 0.4551\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": "{desc}[{n_fmt}/{total_fmt}] {percentage:3.0f}%|{bar}{postfix} [{elapsed}<{remaining}]",
       "colour": null,
       "elapsed": 0.013232231140136719,
       "initial": 1,
       "n": 1,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 126,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[1/126]   1%|           [00:00<?]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2346: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": "{desc}[{n_fmt}/{total_fmt}] {percentage:3.0f}%|{bar}{postfix} [{elapsed}<{remaining}]",
       "colour": null,
       "elapsed": 0.015576839447021484,
       "initial": 1,
       "n": 1,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 43,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[1/43]   2%|2          [00:00<?]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": "{desc}[{n_fmt}/{total_fmt}] {percentage:3.0f}%|{bar}{postfix} [{elapsed}<{remaining}]",
       "colour": null,
       "elapsed": 0.010600566864013672,
       "initial": 1,
       "n": 1,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 5,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[1/5]  20%|##         [00:00<?]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": "{desc}[{n_fmt}/{total_fmt}] {percentage:3.0f}%|{bar}{postfix} [{elapsed}<{remaining}]",
       "colour": null,
       "elapsed": 0.010448455810546875,
       "initial": 1,
       "n": 1,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 79,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[1/79]   1%|1          [00:00<?]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6  Train acc: 0.1585 loss: 0.7096  Val acc: 0.1600 loss: 0.7075 f1: 0.1585  Test acc: 0.1681 loss: 0.7090 f1: 0.1674\n",
      "Epoch: 6  Train acc: 0.1585 loss: 0.7096  Val acc: 0.1600 loss: 0.7075 f1: 0.1585  Test acc: 0.1681 loss: 0.7090 f1: 0.1674\n",
      "Epoch: 6  Train acc: 0.1585 loss: 0.7096  Val acc: 0.1600 loss: 0.7075 f1: 0.1585  Test acc: 0.1681 loss: 0.7090 f1: 0.1674\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": "{desc}[{n_fmt}/{total_fmt}] {percentage:3.0f}%|{bar}{postfix} [{elapsed}<{remaining}]",
       "colour": null,
       "elapsed": 0.010674238204956055,
       "initial": 1,
       "n": 1,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 126,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[1/126]   1%|           [00:00<?]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2346: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": "{desc}[{n_fmt}/{total_fmt}] {percentage:3.0f}%|{bar}{postfix} [{elapsed}<{remaining}]",
       "colour": null,
       "elapsed": 0.012270689010620117,
       "initial": 1,
       "n": 1,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 43,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[1/43]   2%|2          [00:00<?]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": "{desc}[{n_fmt}/{total_fmt}] {percentage:3.0f}%|{bar}{postfix} [{elapsed}<{remaining}]",
       "colour": null,
       "elapsed": 0.011612653732299805,
       "initial": 1,
       "n": 1,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 5,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[1/5]  20%|##         [00:00<?]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": "{desc}[{n_fmt}/{total_fmt}] {percentage:3.0f}%|{bar}{postfix} [{elapsed}<{remaining}]",
       "colour": null,
       "elapsed": 0.010859251022338867,
       "initial": 1,
       "n": 1,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 79,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[1/79]   1%|1          [00:00<?]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7  Train acc: 0.9296 loss: 0.6659  Val acc: 0.9333 loss: 0.6654 f1: 0.9375  Test acc: 0.9144 loss: 0.6673 f1: 0.9150\n",
      "Epoch: 7  Train acc: 0.9296 loss: 0.6659  Val acc: 0.9333 loss: 0.6654 f1: 0.9375  Test acc: 0.9144 loss: 0.6673 f1: 0.9150\n",
      "Epoch: 7  Train acc: 0.9296 loss: 0.6659  Val acc: 0.9333 loss: 0.6654 f1: 0.9375  Test acc: 0.9144 loss: 0.6673 f1: 0.9150\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": "{desc}[{n_fmt}/{total_fmt}] {percentage:3.0f}%|{bar}{postfix} [{elapsed}<{remaining}]",
       "colour": null,
       "elapsed": 0.011739015579223633,
       "initial": 1,
       "n": 1,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 126,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[1/126]   1%|           [00:00<?]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2346: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": "{desc}[{n_fmt}/{total_fmt}] {percentage:3.0f}%|{bar}{postfix} [{elapsed}<{remaining}]",
       "colour": null,
       "elapsed": 0.011681318283081055,
       "initial": 1,
       "n": 1,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 43,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[1/43]   2%|2          [00:00<?]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": "{desc}[{n_fmt}/{total_fmt}] {percentage:3.0f}%|{bar}{postfix} [{elapsed}<{remaining}]",
       "colour": null,
       "elapsed": 0.009928226470947266,
       "initial": 1,
       "n": 1,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 5,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[1/5]  20%|##         [00:00<?]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": "{desc}[{n_fmt}/{total_fmt}] {percentage:3.0f}%|{bar}{postfix} [{elapsed}<{remaining}]",
       "colour": null,
       "elapsed": 0.010495185852050781,
       "initial": 1,
       "n": 1,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 79,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[1/79]   1%|1          [00:00<?]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8  Train acc: 0.4852 loss: 0.7350  Val acc: 0.4333 loss: 0.7464 f1: 0.4233  Test acc: 0.4528 loss: 0.7413 f1: 0.4551\n",
      "Epoch: 8  Train acc: 0.4852 loss: 0.7350  Val acc: 0.4333 loss: 0.7464 f1: 0.4233  Test acc: 0.4528 loss: 0.7413 f1: 0.4551\n",
      "Epoch: 8  Train acc: 0.4852 loss: 0.7350  Val acc: 0.4333 loss: 0.7464 f1: 0.4233  Test acc: 0.4528 loss: 0.7413 f1: 0.4551\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": "{desc}[{n_fmt}/{total_fmt}] {percentage:3.0f}%|{bar}{postfix} [{elapsed}<{remaining}]",
       "colour": null,
       "elapsed": 0.011133670806884766,
       "initial": 1,
       "n": 1,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 126,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[1/126]   1%|           [00:00<?]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2346: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": "{desc}[{n_fmt}/{total_fmt}] {percentage:3.0f}%|{bar}{postfix} [{elapsed}<{remaining}]",
       "colour": null,
       "elapsed": 0.012084007263183594,
       "initial": 1,
       "n": 1,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 43,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[1/43]   2%|2          [00:00<?]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": "{desc}[{n_fmt}/{total_fmt}] {percentage:3.0f}%|{bar}{postfix} [{elapsed}<{remaining}]",
       "colour": null,
       "elapsed": 0.010790824890136719,
       "initial": 1,
       "n": 1,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 5,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[1/5]  20%|##         [00:00<?]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": "{desc}[{n_fmt}/{total_fmt}] {percentage:3.0f}%|{bar}{postfix} [{elapsed}<{remaining}]",
       "colour": null,
       "elapsed": 0.011057138442993164,
       "initial": 1,
       "n": 1,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 79,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[1/79]   1%|1          [00:00<?]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9  Train acc: 0.4852 loss: 0.7135  Val acc: 0.4333 loss: 0.7296 f1: 0.4233  Test acc: 0.4528 loss: 0.7232 f1: 0.4551\n",
      "Epoch: 9  Train acc: 0.4852 loss: 0.7135  Val acc: 0.4333 loss: 0.7296 f1: 0.4233  Test acc: 0.4528 loss: 0.7232 f1: 0.4551\n",
      "Epoch: 9  Train acc: 0.4852 loss: 0.7135  Val acc: 0.4333 loss: 0.7296 f1: 0.4233  Test acc: 0.4528 loss: 0.7232 f1: 0.4551\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": "{desc}[{n_fmt}/{total_fmt}] {percentage:3.0f}%|{bar}{postfix} [{elapsed}<{remaining}]",
       "colour": null,
       "elapsed": 0.009133577346801758,
       "initial": 1,
       "n": 1,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 126,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[1/126]   1%|           [00:00<?]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2346: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": "{desc}[{n_fmt}/{total_fmt}] {percentage:3.0f}%|{bar}{postfix} [{elapsed}<{remaining}]",
       "colour": null,
       "elapsed": 0.011470317840576172,
       "initial": 1,
       "n": 1,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 43,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[1/43]   2%|2          [00:00<?]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": "{desc}[{n_fmt}/{total_fmt}] {percentage:3.0f}%|{bar}{postfix} [{elapsed}<{remaining}]",
       "colour": null,
       "elapsed": 0.008843660354614258,
       "initial": 1,
       "n": 1,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 5,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[1/5]  20%|##         [00:00<?]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": "{desc}[{n_fmt}/{total_fmt}] {percentage:3.0f}%|{bar}{postfix} [{elapsed}<{remaining}]",
       "colour": null,
       "elapsed": 0.009324073791503906,
       "initial": 1,
       "n": 1,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 79,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[1/79]   1%|1          [00:00<?]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10  Train acc: 0.4067 loss: 0.7340  Val acc: 0.4533 loss: 0.7314 f1: 0.4648  Test acc: 0.4405 loss: 0.7310 f1: 0.4379\n",
      "Epoch: 10  Train acc: 0.4067 loss: 0.7340  Val acc: 0.4533 loss: 0.7314 f1: 0.4648  Test acc: 0.4405 loss: 0.7310 f1: 0.4379\n",
      "Epoch: 10  Train acc: 0.4067 loss: 0.7340  Val acc: 0.4533 loss: 0.7314 f1: 0.4648  Test acc: 0.4405 loss: 0.7310 f1: 0.4379\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "State:\n",
       "\titeration: 1260\n",
       "\tepoch: 10\n",
       "\tepoch_length: 126\n",
       "\tmax_epochs: 10\n",
       "\toutput: <class 'tuple'>\n",
       "\tbatch: <class 'list'>\n",
       "\tmetrics: <class 'dict'>\n",
       "\tdataloader: <class 'torch.utils.data.dataloader.DataLoader'>\n",
       "\tseed: <class 'NoneType'>\n",
       "\ttimes: <class 'dict'>"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "\n",
    "@trainer.on(Events.EPOCH_COMPLETED)\n",
    "\n",
    "\n",
    "def log_training_results(trainer):\n",
    "    evaluator.run(idx_loader_train)\n",
    "    metrics = evaluator.state.metrics\n",
    "    train_acc, train_nll = metrics[\"acc\"], metrics[\"nll\"]\n",
    "    evaluator.run(idx_loader_val)\n",
    "    metrics = evaluator.state.metrics\n",
    "    val_acc, val_nll, val_f1 = metrics[\"acc\"], metrics[\"nll\"], metrics[\"f1\"]\n",
    "    evaluator.run(idx_loader_test)\n",
    "    metrics = evaluator.state.metrics\n",
    "    test_acc, test_nll, test_f1 = metrics[\"acc\"], metrics[\"nll\"], metrics[\"f1\"]\n",
    "    logger.info(\n",
    "        \"Epoch: {}  Train acc: {:.4f} loss: {:.4f}  Val acc: {:.4f} loss: {:.4f} f1: {:.4f}  Test acc: {:.4f} loss: {:.4f} f1: {:.4f}\"\n",
    "        .format(trainer.state.epoch, train_acc, train_nll, \n",
    "                val_acc, val_nll, val_f1, \n",
    "                test_acc, test_nll, test_f1\n",
    "               )\n",
    "    )\n",
    "    if test_f1 > log_training_results.best_test_f1 and test_f1 > 0.8712:\n",
    "        logger.info(\"New checkpoint\")\n",
    "        torch.save(\n",
    "            {\n",
    "                'bert_model': model.bert_model.state_dict(),\n",
    "                'classifier': model.classifier.state_dict(),\n",
    "                'gcn': model.gcn.state_dict(),\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "                'epoch': trainer.state.epoch,\n",
    "                'seed':trainer.state.seed,\n",
    "            },\n",
    "            os.path.join(\n",
    "                ckpt_dir, 'checkpoint.pth'\n",
    "            )\n",
    "        )\n",
    "        log_training_results.best_test_f1 = test_f1\n",
    "\n",
    "nb_epochs = 10\n",
    "log_training_results.best_test_f1 = 0\n",
    "g = update_feature()\n",
    "trainer.run(idx_loader, max_epochs=nb_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08dff183-44fb-4faa-a6ee-52458f996d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle要拿掉， 改update 內容\n",
    "# 要知道是哪個doc對到那個 embedding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff356f3a-691d-47c1-ad22-996527e7fcd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d7d6fc-09c2-40ee-b214-78f390897814",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
