{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "387d1f56-38df-4314-97d0-d481d3a20116",
   "metadata": {},
   "source": [
    "#  HyperGAT\n",
    "hypergraph attention networks\n",
    "paper: https://aclanthology.org/2020.emnlp-main.399.pdf\n",
    "\n",
    "github: https://github.com/kaize0409/HyperGAT_TextClassification/tree/main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ed7ff0a-89dc-4bed-971c-af29f066adfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/nm6104054/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9172204-97bd-4f4c-97bd-d50817fbeab8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1137"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "dataset = pd.read_csv('dataset/20ng/20ng_over512.csv', names=['label','text'])\n",
    "dataa = \"20ng\"\n",
    "len(dataset['label']*0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35dd2adc-5dcc-4f00-b1b1-4958b30d4b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from scipy.sparse import coo_array\n",
    "# row  = np.array([0, 0, 1, 3, 1, 0, 0])\n",
    "# col  = np.array([0, 2, 1, 3, 1, 0, 0])\n",
    "# data = np.array([1, 1, 1, 1, 1, 1, 1])\n",
    "# coo = coo_array((data, (row, col)), shape=(4, 5))\n",
    "# # Duplicate indices are maintained until implicitly or explicitly summed\n",
    "# np.max(coo.data)\n",
    "# coo.toarray()\n",
    "# # array([[3, 0, 1, 0],\n",
    "#        [0, 2, 0, 0],\n",
    "#        [0, 0, 0, 0],\n",
    "#        [0, 0, 0, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f85f9d48-11a8-4eb2-8e9e-6a4ac771fcd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_model(model, train_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9014e1b7-52d9-4941-8e1c-de6d25c7426a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c60bff8-be41-4762-bb27-0b75da466bba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/nm6104054/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import networkx as nx\n",
    "import scipy.sparse as sp\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from nltk.wsd import lesk\n",
    "from nltk.corpus import wordnet as wn\n",
    "# from scipy.sparse.linalg.eigen.arpack import eigsh\n",
    "import sys\n",
    "import re\n",
    "import collections\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from multiprocessing import Process, Queue\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim.utils import *\n",
    "\n",
    "\n",
    "\n",
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for all datasets except for SST.\n",
    "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    return string.strip().lower()\n",
    "\n",
    "\n",
    "def clean_str_simple_version(string, dataset):\n",
    "\n",
    "    string = re.sub(r\"\\\\\", \"\", string)\n",
    "    string = re.sub(r\"\\'\", \"\", string)\n",
    "    string = re.sub(r\"\\\"\", \"\", string)\n",
    "    return string.strip().lower()\n",
    "\n",
    "\n",
    "def show_statisctic(clean_docs):\n",
    "    min_len = 10000\n",
    "    aver_len = 0\n",
    "    max_len = 0 \n",
    "    num_sentence = sum([len(i) for i in clean_docs])\n",
    "    ave_num_sentence = num_sentence*1.0/len(clean_docs)\n",
    "\n",
    "    for doc in clean_docs:\n",
    "        for sentence in doc:\n",
    "            temp = sentence\n",
    "            aver_len = aver_len + len(temp)\n",
    "\n",
    "            if len(temp) < min_len:\n",
    "                min_len = len(temp)\n",
    "            if len(temp) > max_len:\n",
    "                max_len = len(temp)\n",
    "\n",
    "    aver_len = 1.0 * aver_len / num_sentence\n",
    "\n",
    "    print('min_len_of_sentence : ' + str(min_len))\n",
    "    print('max_len_of_sentence : ' + str(max_len))\n",
    "    print('min_num_of_sentence : ' + str(min([len(i) for i in clean_docs])))\n",
    "    print('max_num_of_sentence : ' + str(max([len(i) for i in clean_docs])))\n",
    "    print('average_len_of_sentence: ' + str(aver_len))\n",
    "    print('average_num_of_sentence: ' + str(ave_num_sentence))\n",
    "    print('Total_num_of_sentence : ' + str(num_sentence))\n",
    "\n",
    "    return max([len(i) for i in clean_docs])\n",
    "\n",
    "\n",
    "def clean_document(doc_sentence_list, dataset):\n",
    "\n",
    "    stop_words = stopwords.words('english')\n",
    "    stop_words = set(stop_words)\n",
    "    stemmer = WordNetLemmatizer()\n",
    "\n",
    "    word_freq = Counter()\n",
    "\n",
    "    for doc_sentences in doc_sentence_list:\n",
    "        for sentence in doc_sentences:\n",
    "            temp = word_tokenize(clean_str(sentence))\n",
    "            temp = ' '.join([stemmer.lemmatize(word) for word in temp])\n",
    "\n",
    "            words = temp.split()\n",
    "            for word in words:\n",
    "                word_freq[word] += 1\n",
    "\n",
    "    highbar = word_freq.most_common(10)[-1][1]\n",
    "    clean_docs = []\n",
    "    for doc_sentences in doc_sentence_list:\n",
    "        clean_doc = []\n",
    "        count_num = 0\n",
    "        for sentence in doc_sentences:\n",
    "            temp = word_tokenize(clean_str(sentence))\n",
    "            temp = ' '.join([stemmer.lemmatize(word) for word in temp])\n",
    "\n",
    "            words = temp.split()\n",
    "            doc_words = []\n",
    "            for word in words:\n",
    "                # if dataset == 'mr':\n",
    "                #     if not word in stop_words:\n",
    "                #         doc_words.append(word)\n",
    "                # elif (word not in stop_words) and (word_freq[word] >= 5) and (word_freq[word] < highbar):\n",
    "                #     doc_words.append(word)\n",
    "                    \n",
    "                if (word not in stop_words) and (word_freq[word] >= 5) and (word_freq[word] < highbar):\n",
    "                    doc_words.append(word)\n",
    "\n",
    "            clean_doc.append(doc_words)\n",
    "            count_num += len(doc_words)\n",
    "\n",
    "            # if dataset == '20ng' and count_num > 2000:\n",
    "            #     break\n",
    "            \n",
    "        clean_docs.append(clean_doc)\n",
    "\n",
    "    return clean_docs\n",
    "\n",
    "def split_validation(train_set, valid_portion, SEED):\n",
    "    np.random.seed(SEED)\n",
    "\n",
    "    train_set_x = [i for i,j in train_set]\n",
    "    train_set_y = [j for i,j in train_set]\n",
    "\n",
    "    if valid_portion == 0.0:\n",
    "        return (train_set_x, train_set_y)\n",
    "\n",
    "    n_samples = len(train_set_x)\n",
    "    sidx = np.arange(n_samples, dtype='int32')\n",
    "    np.random.shuffle(sidx)\n",
    "    n_train = int(np.round(n_samples * (1. - valid_portion)))\n",
    "    valid_set_x = [train_set_x[s] for s in sidx[n_train:]]\n",
    "    valid_set_y = [train_set_y[s] for s in sidx[n_train:]]\n",
    "    train_set_x = [train_set_x[s] for s in sidx[:n_train]]\n",
    "    train_set_y = [train_set_y[s] for s in sidx[:n_train]]\n",
    "\n",
    "    return (train_set_x, train_set_y), (valid_set_x, valid_set_y)\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "class Data():\n",
    "    def __init__(self, data, max_num_sentence, keywords_dic, num_categories, LDA = False):\n",
    "        inputs = data[0]\n",
    "        self.inputs = np.asarray(inputs) \n",
    "        self.targets = np.asarray(data[1])\n",
    "        self.length = len(inputs)\n",
    "        self.keywords = keywords_dic\n",
    "        self.LDA = LDA\n",
    "        self.num_categories = num_categories\n",
    "\n",
    "    def generate_batch(self, batch_size, shuffle = False):\n",
    "        if shuffle:\n",
    "            shuffled_arg = np.arange(self.length)\n",
    "            np.random.shuffle(shuffled_arg)\n",
    "            self.inputs = self.inputs[shuffled_arg]\n",
    "            self.targets = self.targets[shuffled_arg]\n",
    "\n",
    "        n_batch = int(self.length / batch_size)\n",
    "        if self.length % batch_size != 0:\n",
    "            n_batch += 1\n",
    "        slices = np.split(np.arange(n_batch * batch_size), n_batch)\n",
    "        slices[-1] = slices[-1][:(self.length - batch_size * (n_batch - 1))]\n",
    "        return slices\n",
    "\n",
    "    def get_slice(self, iList):\n",
    "        inputs, targets = self.inputs[iList], self.targets[iList]\n",
    "        items, n_node, HT, alias_inputs, node_masks, node_dic = [], [], [], [], [], []\n",
    "\n",
    "        for u_input in inputs:\n",
    "            temp_s = []\n",
    "            for s in u_input:\n",
    "                temp_s += s\n",
    "            \n",
    "            temp_l = list(set(temp_s))    \n",
    "            temp_dic = {temp_l[i]: i for i in range(len(temp_l))}        \n",
    "            n_node.append(temp_l)\n",
    "            alias_inputs.append([temp_dic[i] for i in temp_s])\n",
    "            node_dic.append(temp_dic)\n",
    "\n",
    "\n",
    "        max_n_node = np.max([len(i) for i in n_node])\n",
    "\n",
    "\n",
    "        num_edge = [len(i) for i in inputs]\n",
    "\n",
    "        if self.LDA:\n",
    "            num_edge = [i + self.num_categories  for i in num_edge]\n",
    "\n",
    "        max_n_edge = max(num_edge)\n",
    "\n",
    "        max_se_len = max([len(i) for i in alias_inputs])\n",
    "\n",
    "        for idx in range(len(inputs)):\n",
    "            u_input = inputs[idx]\n",
    "            node = n_node[idx]\n",
    "            items.append(node + (max_n_node - len(node)) * [0])\n",
    "\n",
    "\n",
    "            rows = []\n",
    "            cols = []\n",
    "            vals = []\n",
    "\n",
    "            \n",
    "            for s in range(len(u_input)):\n",
    "                for i in np.arange(len(u_input[s])):\n",
    "                    if u_input[s][i] == 0:\n",
    "                        continue\n",
    "\n",
    "                    rows.append(node_dic[idx][u_input[s][i]])\n",
    "                    cols.append(s)\n",
    "                    vals.append(1.0)\n",
    "\n",
    "            if len(cols) == 0:\n",
    "                s = 0\n",
    "            else:\n",
    "                s = max(cols) + 1\n",
    "\n",
    "            if self.LDA:\n",
    "                for i in node:\n",
    "                    if i in self.keywords:\n",
    "                        temp = self.keywords[i]\n",
    "                                                \n",
    "                        rows += [node_dic[idx][i]]*len(temp)\n",
    "                        cols += [topic + s for topic in temp]\n",
    "                        vals += [1.0]*len(temp)\n",
    "                                \n",
    "\n",
    "            u_H = sp.coo_matrix((vals, (rows, cols)), shape=(max_n_node, max_n_edge))\n",
    "            HT.append(np.asarray(u_H.T.todense()))\n",
    "            \n",
    "            alias_inputs[idx] = [j for j in range(max_n_node)]\n",
    "            node_masks.append([1 for j in node] + (max_n_node - len(node)) * [0])\n",
    "\n",
    "\n",
    "        return alias_inputs, HT, items, targets, node_masks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6a179f0-53a4-4e51-a640-2ef4fdef3dcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "drive db scsi disk bit window card hard controller mov\n",
      "Topic 1:\n",
      "nazi hitler german limbaugh jew germany page nlns liberal chancellor\n",
      "Topic 2:\n",
      "archiving facto drake concert edu warrant wa com people os2\n",
      "Topic 3:\n",
      "wa ha people data conclusion know absolute argument religion remote\n",
      "Topic 4:\n",
      "armenian wa turkish people greek turk turkey said armenia muslim\n",
      "Topic 5:\n",
      "space nasa mission satellite earth orbit gov shuttle probe launch\n",
      "Topic 6:\n",
      "god jesus christian wa say bible people doe believe life\n",
      "Topic 7:\n",
      "25 10 55 16 11 14 12 13 15 17\n",
      "Topic 8:\n",
      "church father catholic pope holy son spirit maria schism bishop\n",
      "Topic 9:\n",
      "key encryption chip clipper bit security enforcement escrow algorithm device\n",
      "Topic 10:\n",
      "00 edu com 50 1st appears 10 25 copy 40\n",
      "Topic 11:\n",
      "wa ha people edu dont like time think com year\n",
      "Topic 12:\n",
      "team player game ca hockey season play gm playoff fan\n",
      "Topic 13:\n",
      "plane den point p3 p2 radius p1 sphere p13 p23\n",
      "Topic 14:\n",
      "tulkarm torture shabak detention interrogation intercon amanda polygon omar installation\n",
      "Topic 15:\n",
      "food patient doctor msg candida study disease pain yeast vitamin\n",
      "Topic 16:\n",
      "weaver harris ahmadiyya randy spence idaho fadeley statesman rana ktvb\n",
      "Topic 17:\n",
      "ax max pl 1d9 b8f wm 3t g9v 0t bxn\n",
      "Topic 18:\n",
      "edu file available program use image ftp version information mail\n",
      "Topic 19:\n",
      "israel israeli arab jew right wa state palestinian land attack\n",
      "181\n",
      "{'drive': [0], 'db': [0], 'scsi': [0], 'disk': [0], 'bit': [0, 9], 'window': [0], 'card': [0], 'hard': [0], 'controller': [0], 'mov': [0], 'nazi': [1], 'hitler': [1], 'german': [1], 'limbaugh': [1], 'jew': [1, 19], 'germany': [1], 'page': [1], 'nlns': [1], 'liberal': [1], 'chancellor': [1], 'archiving': [2], 'facto': [2], 'drake': [2], 'concert': [2], 'edu': [2, 10, 11, 18], 'warrant': [2], 'wa': [2, 3, 4, 6, 11, 19], 'com': [2, 10, 11], 'people': [2, 3, 4, 6, 11], 'os2': [2], 'ha': [3, 11], 'data': [3], 'conclusion': [3], 'know': [3], 'absolute': [3], 'argument': [3], 'religion': [3], 'remote': [3], 'armenian': [4], 'turkish': [4], 'greek': [4], 'turk': [4], 'turkey': [4], 'said': [4], 'armenia': [4], 'muslim': [4], 'space': [5], 'nasa': [5], 'mission': [5], 'satellite': [5], 'earth': [5], 'orbit': [5], 'gov': [5], 'shuttle': [5], 'probe': [5], 'launch': [5], 'god': [6], 'jesus': [6], 'christian': [6], 'say': [6], 'bible': [6], 'doe': [6], 'believe': [6], 'life': [6], '25': [7, 10], '10': [7, 10], '55': [7], '16': [7], '11': [7], '14': [7], '12': [7], '13': [7], '15': [7], '17': [7], 'church': [8], 'father': [8], 'catholic': [8], 'pope': [8], 'holy': [8], 'son': [8], 'spirit': [8], 'maria': [8], 'schism': [8], 'bishop': [8], 'key': [9], 'encryption': [9], 'chip': [9], 'clipper': [9], 'security': [9], 'enforcement': [9], 'escrow': [9], 'algorithm': [9], 'device': [9], '00': [10], '50': [10], '1st': [10], 'appears': [10], 'copy': [10], '40': [10], 'dont': [11], 'like': [11], 'time': [11], 'think': [11], 'year': [11], 'team': [12], 'player': [12], 'game': [12], 'ca': [12], 'hockey': [12], 'season': [12], 'play': [12], 'gm': [12], 'playoff': [12], 'fan': [12], 'plane': [13], 'den': [13], 'point': [13], 'p3': [13], 'p2': [13], 'radius': [13], 'p1': [13], 'sphere': [13], 'p13': [13], 'p23': [13], 'tulkarm': [14], 'torture': [14], 'shabak': [14], 'detention': [14], 'interrogation': [14], 'intercon': [14], 'amanda': [14], 'polygon': [14], 'omar': [14], 'installation': [14], 'food': [15], 'patient': [15], 'doctor': [15], 'msg': [15], 'candida': [15], 'study': [15], 'disease': [15], 'pain': [15], 'yeast': [15], 'vitamin': [15], 'weaver': [16], 'harris': [16], 'ahmadiyya': [16], 'randy': [16], 'spence': [16], 'idaho': [16], 'fadeley': [16], 'statesman': [16], 'rana': [16], 'ktvb': [16], 'ax': [17], 'max': [17], 'pl': [17], '1d9': [17], 'b8f': [17], 'wm': [17], '3t': [17], 'g9v': [17], '0t': [17], 'bxn': [17], 'file': [18], 'available': [18], 'program': [18], 'use': [18], 'image': [18], 'ftp': [18], 'version': [18], 'information': [18], 'mail': [18], 'israel': [19], 'israeli': [19], 'arab': [19], 'right': [19], 'state': [19], 'palestinian': [19], 'land': [19], 'attack': [19]}\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from sklearn.utils import class_weight\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from nltk.wsd import lesk\n",
    "from nltk.corpus import wordnet as wn\n",
    "# from utils import clean_str, clean_str_simple_version, show_statisctic, clean_document\n",
    "import sys\n",
    "from nltk import tokenize\n",
    "import collections\n",
    "from collections import Counter\n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "\n",
    "\n",
    "def display_topics(model, feature_names, no_top_words):\n",
    "    keywords_dic = {}\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic %d:\" % (topic_idx))\n",
    "        klist = [feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]\n",
    "        print(\" \".join(klist))\n",
    "        for k in klist:\n",
    "            if not k in keywords_dic:\n",
    "                keywords_dic[k] = []\n",
    "            keywords_dic[k].append(topic_idx)\n",
    "    return keywords_dic\n",
    "\n",
    "def Generate_LDA(data,topics):\n",
    "    dataset = pd.read_csv('dataset/20ng/20ng_over512.csv', names=['label','text'])\n",
    "    dataset['text']\n",
    "    doc_content_list = []\n",
    "    doc_sentence_list = []\n",
    "    # f = open('dataa/' + dataset + '_corpus.txt', 'rb')\n",
    "    for line in dataset['text']:\n",
    "        doc_content_list.append(line)\n",
    "        doc_sentence_list.append(tokenize.sent_tokenize(clean_str_simple_version(doc_content_list[-1], dataset)))\n",
    "    # f.close()\n",
    "\n",
    "    # Remove the rare words\n",
    "    doc_content_list = clean_document(doc_sentence_list, dataset)\n",
    "\n",
    "    doc_list = []\n",
    "    for doc in doc_content_list:\n",
    "        temp = ''\n",
    "        for sen in doc:\n",
    "            temp += (' '.join(sen) + ' ')\n",
    "        doc_list.append(temp)\n",
    "\n",
    "    vectorizer = CountVectorizer(stop_words='english',max_df=0.98)\n",
    "    vector = vectorizer.fit_transform(doc_list)\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    lda = LatentDirichletAllocation(n_components=topics,learning_method='online',learning_offset=50.,random_state=0).fit(vector)\n",
    "\n",
    "    keywords_dic = display_topics(lda, feature_names, 10)\n",
    "    print(len(keywords_dic))\n",
    "    print(keywords_dic)\n",
    "\n",
    "    pickle.dump(keywords_dic, open('data/' + data+ '_LDA.p', \"wb\" ) )\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # parser = argparse.ArgumentParser()\n",
    "    # parser.add_argument('--dataset', default='20ng', help='dataset name: 20ng/R8/R52/ohsumed/mr')\n",
    "    # parser.add_argument('--topn', type=int, default=10, help='top n keywords')\n",
    "    # parser.add_argument('--topics', type=int, help='number of topics')\n",
    "    topics = 20\n",
    "    topn = 10 \n",
    "    # args = parser.parse_args()\n",
    "    # print(args)\n",
    "    data = \"20ng\"\n",
    "    Generate_LDA(data,topics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71b433fc-2072-41b6-9460-db54856dcd27",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from nltk.wsd import lesk\n",
    "from nltk.corpus import wordnet as wn\n",
    "# from utils import clean_str, show_statisctic, clean_document, clean_str_simple_version\n",
    "import collections\n",
    "from collections import Counter\n",
    "import random\n",
    "import numpy as np\n",
    "import pickle\n",
    "import json\n",
    "from nltk import tokenize\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "def read_file(dataname, LDA=True):\n",
    "\n",
    "    doc_content_list = []\n",
    "    doc_sentence_list = []\n",
    "#     f = open('data/' + dataset + '_corpus.txt', 'rb')\n",
    "\n",
    "#     for line in f.readlines():\n",
    "#         doc_content_list.append(line.strip().decode('latin1'))\n",
    "#         doc_sentence_list.append(tokenize.sent_tokenize(clean_str_simple_version(doc_content_list[-1], dataset)))\n",
    "#     f.close()\n",
    "\n",
    "    dataset = pd.read_csv('dataset/20ng/20ng_over512.csv', names=['label','text'])\n",
    "    dataset['text']\n",
    "    doc_content_list = []\n",
    "    doc_sentence_list = []\n",
    "    # f = open('dataa/' + dataset + '_corpus.txt', 'rb')\n",
    "    for line in dataset['text']:\n",
    "        doc_content_list.append(line)\n",
    "        doc_sentence_list.append(tokenize.sent_tokenize(clean_str_simple_version(doc_content_list[-1], dataname)))\n",
    "\n",
    "\n",
    "    doc_content_list = clean_document(doc_sentence_list, dataset)\n",
    "\n",
    "    max_num_sentence = show_statisctic(doc_content_list)\n",
    "\n",
    "    doc_train_list_original = []\n",
    "    doc_test_list_original = []\n",
    "    labels_dic = {}\n",
    "    label_count = Counter()\n",
    "\n",
    "    # 改掉\n",
    "    i = 0\n",
    "    LE = LabelEncoder()\n",
    "    a=int(len(dataset['label']))*0.5\n",
    "    dataset['label'] = LE.fit_transform(dataset['label'])\n",
    "    for index, temp in enumerate(dataset['label']):\n",
    "        if index < a : \n",
    "            doc_train_list_original.append((doc_content_list[i],temp))\n",
    "        else :\n",
    "            doc_test_list_original.append((doc_content_list[i],temp))\n",
    "        if not temp in labels_dic:\n",
    "            labels_dic[temp] = len(labels_dic)\n",
    "        i += 1    \n",
    "#     f = open('data/' + dataset + '_labels.txt', 'r')\n",
    "#     lines = f.readlines()\n",
    "#     for line in lines:\n",
    "#         temp = line.strip().split(\"\\t\")\n",
    "#         if temp[1].find('test') != -1:\n",
    "#             doc_test_list_original.append((doc_content_list[i],temp[2]))\n",
    "#         elif temp[1].find('train') != -1:\n",
    "#             doc_train_list_original.append((doc_content_list[i],temp[2]))\n",
    "        # if not temp in labels_dic:\n",
    "        #     labels_dic[temp[2]] = len(labels_dic)\n",
    "#         label_count[temp[2]] += 1\n",
    "#        \n",
    "\n",
    "#     f.close()\n",
    "#     print(label_count)\n",
    "\n",
    "\n",
    "\n",
    "    word_freq = Counter()\n",
    "    word_set = set()\n",
    "    for doc_words in doc_content_list:\n",
    "        for words in doc_words:\n",
    "            for word in words:\n",
    "                word_set.add(word)\n",
    "                word_freq[word] += 1\n",
    "\n",
    "    vocab = list(word_set)\n",
    "    vocab_size = len(vocab)\n",
    "\n",
    "    vocab_dic = {}\n",
    "    for i in word_set:\n",
    "        vocab_dic[i] = len(vocab_dic) + 1\n",
    "\n",
    "    print('Total_number_of_words: ' + str(len(vocab)))\n",
    "    print('Total_number_of_categories: ' + str(len(labels_dic)))\n",
    "\n",
    "    doc_train_list = []\n",
    "    doc_test_list = []\n",
    "\n",
    "    for doc,label in doc_train_list_original:\n",
    "        temp_doc = []\n",
    "        for sentence in doc:\n",
    "            temp = []\n",
    "            for word in sentence:\n",
    "                temp.append(vocab_dic[word])\n",
    "            temp_doc.append(temp)\n",
    "        doc_train_list.append((temp_doc,labels_dic[label]))\n",
    "\n",
    "    for doc,label in doc_test_list_original:\n",
    "        temp_doc = []\n",
    "        for sentence in doc:\n",
    "            temp = []\n",
    "            for word in sentence:\n",
    "                temp.append(vocab_dic[word])\n",
    "            temp_doc.append(temp)\n",
    "        doc_test_list.append((temp_doc,labels_dic[label]))\n",
    "\n",
    "    keywords_dic = {}\n",
    "    if LDA:\n",
    "        keywords_dic_original = pickle.load(open('data/' + dataname + '_LDA.p', \"rb\" ))\n",
    "\n",
    "        for i in keywords_dic_original:\n",
    "            if i in vocab_dic:\n",
    "                keywords_dic[vocab_dic[i]] = keywords_dic_original[i]\n",
    "\n",
    "    train_set_y = [j for i,j in doc_train_list]\n",
    "\n",
    "    class_weights = class_weight.compute_class_weight(class_weight='balanced', classes=np.unique(train_set_y),y=train_set_y)\n",
    "\n",
    "    return doc_content_list, doc_train_list, doc_test_list, vocab_dic, labels_dic, max_num_sentence, keywords_dic, class_weights\n",
    "\n",
    "\n",
    "def loadGloveModel(gloveFile, vocab_dic, matrix_len):\n",
    "    print(\"Loading Glove Model\")\n",
    "    f = open(gloveFile,'r')\n",
    "    gloveModel = {}\n",
    "    glove_embedding_dimension = 0\n",
    "    for line in f:\n",
    "        splitLine = line.split()\n",
    "        word = splitLine[0]\n",
    "        glove_embedding_dimension = len(splitLine[1:])\n",
    "        embedding = np.array([float(val) for val in splitLine[1:]])\n",
    "        gloveModel[word] = embedding\n",
    "\n",
    "    words_found = 0\n",
    "    weights_matrix = np.zeros((matrix_len, glove_embedding_dimension))\n",
    "    weights_matrix[0] = np.zeros((glove_embedding_dimension, ))\n",
    "\n",
    "    for word in vocab_dic:\n",
    "        if word in gloveModel:\n",
    "            weights_matrix[vocab_dic[word]] = gloveModel[word]\n",
    "            words_found += 1\n",
    "        else:\n",
    "            weights_matrix[vocab_dic[word]] = gloveModel['the']\n",
    "\n",
    "    print(\"Total \", len(vocab_dic), \" words\")\n",
    "    print(\"Done.\",words_found,\" words loaded from\", gloveFile)\n",
    "\n",
    "    return weights_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c29eb94-6956-42b8-947c-07f369b878f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "\n",
    "class HyperGraphAttentionLayerSparse(nn.Module):\n",
    "\n",
    "    def __init__(self, in_features, out_features, dropout, alpha, transfer, concat=True, bias=False):\n",
    "        super(HyperGraphAttentionLayerSparse, self).__init__()\n",
    "        self.dropout = dropout\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.alpha = alpha\n",
    "        self.concat = concat\n",
    "\n",
    "        self.transfer = transfer\n",
    "\n",
    "        if self.transfer:\n",
    "            self.weight = Parameter(torch.Tensor(self.in_features, self.out_features))\n",
    "        else:\n",
    "            self.register_parameter('weight', None)\n",
    "\n",
    "        self.weight2 = Parameter(torch.Tensor(self.in_features, self.out_features))\n",
    "        self.weight3 = Parameter(torch.Tensor(self.out_features, self.out_features))\n",
    "\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.Tensor(self.out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "        self.word_context = nn.Embedding(1, self.out_features)\n",
    "      \n",
    "        self.a = nn.Parameter(torch.zeros(size=(2*out_features, 1)))   \n",
    "        self.a2 = nn.Parameter(torch.zeros(size=(2*out_features, 1)))        \n",
    "        self.leakyrelu = nn.LeakyReLU(self.alpha)\n",
    "        \n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.out_features)\n",
    "        if self.weight is not None:\n",
    "            self.weight.data.uniform_(-stdv, stdv)\n",
    "        self.weight2.data.uniform_(-stdv, stdv)\n",
    "        self.weight3.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "        \n",
    "        nn.init.uniform_(self.a.data, -stdv, stdv)\n",
    "        nn.init.uniform_(self.a2.data, -stdv, stdv)\n",
    "        nn.init.uniform_(self.word_context.weight.data, -stdv, stdv)\n",
    "\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x_4att = x.matmul(self.weight2)\n",
    "\n",
    "        if self.transfer:\n",
    "            x = x.matmul(self.weight)\n",
    "            if self.bias is not None:\n",
    "                x = x + self.bias        \n",
    "\n",
    "        N1 = adj.shape[1] #number of edge\n",
    "        N2 = adj.shape[2] #number of node\n",
    "\n",
    "        pair = adj.nonzero().t()        \n",
    "\n",
    "        get = lambda i: x_4att[i][adj[i].nonzero().t()[1]]\n",
    "        x1 = torch.cat([get(i) for i in torch.arange(x.shape[0]).long()])\n",
    "\n",
    "\n",
    "        q1 = self.word_context.weight[0:].view(1, -1).repeat(x1.shape[0],1).view(x1.shape[0], self.out_features)\n",
    "        \n",
    "        pair_h = torch.cat((q1, x1), dim=-1)\n",
    "        pair_e = self.leakyrelu(torch.matmul(pair_h, self.a).squeeze()).t()\n",
    "        assert not torch.isnan(pair_e).any()\n",
    "        pair_e = F.dropout(pair_e, self.dropout, training=self.training)\n",
    "\n",
    "        e = torch.sparse_coo_tensor(pair, pair_e, torch.Size([x.shape[0], N1, N2])).to_dense()\n",
    "\n",
    "        zero_vec = -9e15*torch.ones_like(e)\n",
    "        attention = torch.where(adj > 0, e, zero_vec)\n",
    "\n",
    "\n",
    "        attention_edge = F.softmax(attention, dim=2)\n",
    "\n",
    "        edge = torch.matmul(attention_edge, x)\n",
    "        \n",
    "        edge = F.dropout(edge, self.dropout, training=self.training)\n",
    "\n",
    "        edge_4att = edge.matmul(self.weight3)\n",
    "\n",
    "        get = lambda i: edge_4att[i][adj[i].nonzero().t()[0]]\n",
    "        y1 = torch.cat([get(i) for i in torch.arange(x.shape[0]).long()])\n",
    "\n",
    "        get = lambda i: x_4att[i][adj[i].nonzero().t()[1]]\n",
    "        q1 = torch.cat([get(i) for i in torch.arange(x.shape[0]).long()])\n",
    "\n",
    "        pair_h = torch.cat((q1, y1), dim=-1)\n",
    "        pair_e = self.leakyrelu(torch.matmul(pair_h, self.a2).squeeze()).t()\n",
    "        assert not torch.isnan(pair_e).any()\n",
    "        pair_e = F.dropout(pair_e, self.dropout, training=self.training)\n",
    "\n",
    "        e = torch.sparse_coo_tensor(pair, pair_e, torch.Size([x.shape[0], N1, N2])).to_dense()\n",
    "\n",
    "        zero_vec = -9e15*torch.ones_like(e)\n",
    "        attention = torch.where(adj > 0, e, zero_vec)\n",
    "\n",
    "        attention_node = F.softmax(attention.transpose(1,2), dim=2)\n",
    "\n",
    "        node = torch.matmul(attention_node, edge)\n",
    "\n",
    "\n",
    "        if self.concat:\n",
    "            node = F.elu(node)\n",
    "\n",
    "        return node\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' + str(self.in_features) + ' -> ' + str(self.out_features) + ')'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fcc177d6-74c9-4bde-a0fb-faaf580ca62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import Module, Parameter\n",
    "import torch.nn.functional as F\n",
    "from sklearn import metrics\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def trans_to_cuda(variable):\n",
    "    if torch.cuda.is_available():\n",
    "        return variable.cuda()\n",
    "    else:\n",
    "        return variable\n",
    "\n",
    "\n",
    "def trans_to_cpu(variable):\n",
    "    if torch.cuda.is_available():\n",
    "        return variable.cpu()\n",
    "    else:\n",
    "        return variable\n",
    "\n",
    "\n",
    "class HGNN_ATT(nn.Module):\n",
    "    def __init__(self, input_size, n_hid, output_size, dropout=0.3):\n",
    "        super(HGNN_ATT, self).__init__()\n",
    "        self.dropout = dropout\n",
    "        self.gat1 = HyperGraphAttentionLayerSparse(input_size, n_hid, dropout=self.dropout, alpha=0.2, transfer = False, concat=True)\n",
    "        self.gat2 = HyperGraphAttentionLayerSparse(n_hid, output_size, dropout=self.dropout, alpha=0.2, transfer = True, concat=False)\n",
    "        \n",
    "    def forward(self, x, H):   \n",
    "        x = self.gat1(x, H)\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = self.gat2(x, H)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class DocumentGraph(Module):\n",
    "    def __init__(self, pre_trained_weight, class_weights, n_node, n_categories):\n",
    "        super(DocumentGraph, self).__init__()\n",
    "        epoch = 10; lr = 0.005 ; dropout = 0.3; lr_dc = 0.1 ; lr_dc_step = 3; l2 = 1e-6 ; valid_portion =0.1; rand = 1234; normalization = True\n",
    "        self.hidden_size =100\n",
    "        self.n_node = n_node\n",
    "        self.n_categories = n_categories\n",
    "        self.batch_size = 8\n",
    "        self.dropout = 0.3   \n",
    "        self.initial_feature = 300\n",
    "        self.normalization = True\n",
    "        # self.dataset = opt.dataset\n",
    "\n",
    "\n",
    "        self.embedding = nn.Embedding(self.n_node+1, self.initial_feature, padding_idx=0)\n",
    "        self.layer_normH = nn.LayerNorm(self.hidden_size, eps=1e-6)\n",
    "        if self.normalization:\n",
    "            self.layer_normC = nn.LayerNorm(self.n_categories, eps=1e-6)\n",
    "\n",
    "        self.prediction_transform = nn.Linear(self.hidden_size, self.n_categories, bias=True)  \n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "        # if opt.dataset == 'mr':\n",
    "        #     pre_trained_weight = torch.FloatTensor(pre_trained_weight)\n",
    "        #     self.embedding = nn.Embedding.from_pretrained(pre_trained_weight, freeze = False, padding_idx = 0)\n",
    "\n",
    "        self.hgnn = HGNN_ATT(self.initial_feature, self.initial_feature, self.hidden_size, dropout = self.dropout)\n",
    "\n",
    "        self.class_weights = class_weights\n",
    "        self.loss_function = nn.CrossEntropyLoss(weight = trans_to_cuda(torch.Tensor(self.class_weights).float()))\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=lr, weight_decay=l2)\n",
    "        self.scheduler = torch.optim.lr_scheduler.StepLR(self.optimizer, step_size=lr_dc_step, gamma=lr_dc)\n",
    "        \n",
    "        \n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1.0 / math.sqrt(self.hidden_size)\n",
    "        for weight in self.parameters():\n",
    "            weight.data.uniform_(-stdv, stdv)\n",
    "        \n",
    "\n",
    "    def compute_scores(self, inputs, node_masks):\n",
    "        \n",
    "        hidden = inputs * node_masks.view(node_masks.shape[0], -1, 1).float()\n",
    "        b = torch.sum(hidden * node_masks.view(node_masks.shape[0], -1, 1).float(),-2)/torch.sum(node_masks,-1).repeat(hidden.shape[2],1).transpose(0,1)          \n",
    "        b = self.layer_normH(b)           \n",
    "        b = self.prediction_transform(b)\n",
    "\n",
    "        pred = b\n",
    "\n",
    "        if self.normalization:\n",
    "            pred = self.layer_normC(b)\n",
    "        \n",
    "        return pred\n",
    "\n",
    "    def forward(self, inputs, HT):\n",
    "        \n",
    "        hidden = self.embedding(inputs)\n",
    "        nodes = self.hgnn(hidden, HT)\n",
    "        return nodes\n",
    "\n",
    "\n",
    "def forward(model, alias_inputs, HT, items, targets, node_masks):\n",
    "\n",
    "    alias_inputs = trans_to_cuda(torch.Tensor(alias_inputs).long())\n",
    "    items = trans_to_cuda(torch.Tensor(items).long())\n",
    "    HT = trans_to_cuda(torch.Tensor(HT).float())\n",
    "    node_masks = trans_to_cuda(torch.Tensor(node_masks).float())\n",
    "    node = model(items, HT)\n",
    "    get = lambda i: node[i][alias_inputs[i]]\n",
    "    seq_hidden = torch.stack([get(i) for i in torch.arange(len(alias_inputs)).long()])\n",
    "    return targets, model.compute_scores(seq_hidden, node_masks)\n",
    "\n",
    "\n",
    "def train_model(model, train_data):\n",
    "    lr = 0.001 ; dropout = 0.3; lr_dc = 0.1 ; lr_dc_step = 3; l2 = 1e-6 ; valid_portion =0.1; rand = 1234; normalization = True; use_LDA =True\n",
    "\n",
    "    model.scheduler.step()\n",
    "    print('start training: \\n')\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    slices = train_data.generate_batch(batchSize, True)\n",
    "    for step in tqdm(range(len(slices)), total=len(slices), ncols=70, leave=False, unit='b'):\n",
    "        i = slices[step]\n",
    "        alias_inputs, HT, items, targets, node_masks = train_data.get_slice(i)\n",
    "        model.optimizer.zero_grad()\n",
    "        targets, scores = forward(model, alias_inputs, HT, items, targets, node_masks)    \n",
    "        targets = trans_to_cuda(torch.Tensor(targets).long())\n",
    "        loss = model.loss_function(scores, targets)\n",
    "        loss.backward()\n",
    "        model.optimizer.step()\n",
    "        total_loss += loss\n",
    "\n",
    "    print('\\tLoss:\\t%.4f' % (total_loss))\n",
    "\n",
    "\n",
    "\n",
    "def test_model(model, test_data, verbose=True):\n",
    "    model.eval()\n",
    "\n",
    "    test_pred = []\n",
    "    test_labels = []\n",
    "    slices = test_data.generate_batch(10, False)\n",
    "    for step in tqdm(range(len(slices)), total=len(slices), ncols=70, leave=False, unit='b'):\n",
    "        i = slices[step]\n",
    "        alias_inputs, HT, items, targets, node_masks = test_data.get_slice(i)\n",
    "        targets, scores = forward(model, alias_inputs, HT, items, targets, node_masks)\n",
    "        pre_indices = scores.topk(1)[1]\n",
    "        test_labels += list(targets)\n",
    "        test_pred += list(trans_to_cpu(pre_indices).detach().numpy())\n",
    "\n",
    "    details = metrics.classification_report(test_labels, test_pred, digits=4)\n",
    "    acc = metrics.accuracy_score(test_labels, test_pred)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Test Precision, Recall and F1-Score...\")\n",
    "        print(metrics.classification_report(test_labels, test_pred, digits=4))\n",
    "        print(\"Macro average Test Precision, Recall and F1-Score...\")\n",
    "        print(metrics.precision_recall_fscore_support(test_labels, test_pred, average='macro'))\n",
    "        print(\"Micro average Test Precision, Recall and F1-Score...\")\n",
    "        print(metrics.precision_recall_fscore_support(test_labels, test_pred, average='micro'))\n",
    "\n",
    "    return details,acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b025536b-3c85-47ba-886e-374f5bc6123c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_model(model, train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83fe3e49-7755-4fd9-9a61-d13da0268cbb",
   "metadata": {},
   "source": [
    "## Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7830c55a-6c89-4268-9042-61ecbbea482f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min_len_of_sentence : 0\n",
      "max_len_of_sentence : 7265\n",
      "min_num_of_sentence : 1\n",
      "max_num_of_sentence : 405\n",
      "average_len_of_sentence: 11.473086453744493\n",
      "average_num_of_sentence: 51.109938434476696\n",
      "Total_num_of_sentence : 58112\n",
      "Total_number_of_words: 13428\n",
      "Total_number_of_categories: 20\n",
      "Loading Glove Model\n",
      "Total  13428  words\n",
      "Done. 12580  words loaded from glove.6B.300d.txt\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import pickle\n",
    "import time\n",
    "import numpy as np\n",
    "# from utils import split_validation, Data\n",
    "# from preprocess import *\n",
    "# from model import *\n",
    "from sklearn.utils import class_weight\n",
    "import random\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings('ignore') \n",
    "\n",
    "\n",
    "dataname = \"20ng\"\n",
    "batchSize = 8;hiddenSize = 100;initialFeatureSize = 300\n",
    "epochs = 10; lr = 0.001 ; dropout = 0.3; lr_dc = 0.1 ; lr_dc_step = 3; l2 = 1e-6 ; valid_portion =0.1; rand = 1234; normalization = True\n",
    "use_LDA = False\n",
    "\n",
    "\n",
    "\n",
    "SEED =rand\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "\n",
    "\n",
    "# def main():\n",
    "doc_content_list, doc_train_list, doc_test_list, vocab_dic, labels_dic, max_num_sentence, keywords_dic, class_weights = read_file(dataname, True)\n",
    "\n",
    "pre_trained_weight = []\n",
    "gloveFile = 'glove.6B.300d.txt'  \n",
    "\n",
    "pre_trained_weight = loadGloveModel(gloveFile, vocab_dic, len(vocab_dic)+1)\n",
    "\n",
    "train_data, valid_data = split_validation(doc_train_list, valid_portion, SEED)\n",
    "test_data = split_validation(doc_test_list, 0.0, SEED)\n",
    "train_data, valid_data\n",
    "\n",
    "num_categories = len(labels_dic)\n",
    "\n",
    "train_data = Data(train_data, max_num_sentence, keywords_dic, num_categories, use_LDA)\n",
    "valid_data = Data(valid_data, max_num_sentence, keywords_dic, num_categories, use_LDA)\n",
    "test_data = Data(test_data, max_num_sentence,  keywords_dic, num_categories, use_LDA)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3f722bff-0999-4af9-b0d3-f24cb8b6f259",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------\n",
      "epoch:  0\n",
      "start training: \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss:\t181.4580\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy:\t0.5965, Test Accuracy:\t0.6813\n",
      "-------------------------------------------------------\n",
      "epoch:  1\n",
      "start training: \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss:\t136.0115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy:\t0.8421, Test Accuracy:\t0.8380\n",
      "-------------------------------------------------------\n",
      "epoch:  2\n",
      "start training: \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss:\t105.4820\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy:\t0.8421, Test Accuracy:\t0.8504\n",
      "-------------------------------------------------------\n",
      "epoch:  3\n",
      "start training: \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss:\t97.2219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy:\t0.8421, Test Accuracy:\t0.8644\n",
      "-------------------------------------------------------\n",
      "epoch:  4\n",
      "start training: \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss:\t91.3640\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy:\t0.8421, Test Accuracy:\t0.8627\n",
      "-------------------------------------------------------\n",
      "epoch:  5\n",
      "start training: \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss:\t89.2881\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy:\t0.8421, Test Accuracy:\t0.8662\n",
      "-------------------------------------------------------\n",
      "epoch:  6\n",
      "start training: \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss:\t88.6293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy:\t0.8421, Test Accuracy:\t0.8644\n",
      "-------------------------------------------------------\n",
      "epoch:  7\n",
      "start training: \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss:\t88.0786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy:\t0.8596, Test Accuracy:\t0.8715\n",
      "-------------------------------------------------------\n",
      "epoch:  8\n",
      "start training: \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss:\t87.4857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy:\t0.8596, Test Accuracy:\t0.8715\n",
      "-------------------------------------------------------\n",
      "epoch:  9\n",
      "start training: \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss:\t88.3067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy:\t0.8596, Test Accuracy:\t0.8697\n",
      "-------------------------------------------------------\n",
      "epoch:  10\n",
      "start training: \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss:\t87.2032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy:\t0.8596, Test Accuracy:\t0.8697\n",
      "-------------------------------------------------------\n",
      "epoch:  11\n",
      "start training: \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss:\t87.2526\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy:\t0.8596, Test Accuracy:\t0.8697\n",
      "-------------------------------------------------------\n",
      "epoch:  12\n",
      "start training: \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss:\t88.5505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [17]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m train_model(model, train_data)\n\u001b[1;32m      9\u001b[0m valid_detail, valid_acc \u001b[38;5;241m=\u001b[39m test_model(model, valid_data, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m---> 10\u001b[0m detail, acc \u001b[38;5;241m=\u001b[39m \u001b[43mtest_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mValidation Accuracy:\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m%.4f\u001b[39;00m\u001b[38;5;124m, Test Accuracy:\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m%.4f\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m%\u001b[39m (valid_acc,acc))\n",
      "Input \u001b[0;32mIn [14]\u001b[0m, in \u001b[0;36mtest_model\u001b[0;34m(model, test_data, verbose)\u001b[0m\n\u001b[1;32m    146\u001b[0m i \u001b[38;5;241m=\u001b[39m slices[step]\n\u001b[1;32m    147\u001b[0m alias_inputs, HT, items, targets, node_masks \u001b[38;5;241m=\u001b[39m test_data\u001b[38;5;241m.\u001b[39mget_slice(i)\n\u001b[0;32m--> 148\u001b[0m targets, scores \u001b[38;5;241m=\u001b[39m \u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malias_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mHT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mitems\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode_masks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    149\u001b[0m pre_indices \u001b[38;5;241m=\u001b[39m scores\u001b[38;5;241m.\u001b[39mtopk(\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    150\u001b[0m test_labels \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(targets)\n",
      "Input \u001b[0;32mIn [14]\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, alias_inputs, HT, items, targets, node_masks)\u001b[0m\n\u001b[1;32m    106\u001b[0m alias_inputs \u001b[38;5;241m=\u001b[39m trans_to_cuda(torch\u001b[38;5;241m.\u001b[39mTensor(alias_inputs)\u001b[38;5;241m.\u001b[39mlong())\n\u001b[1;32m    107\u001b[0m items \u001b[38;5;241m=\u001b[39m trans_to_cuda(torch\u001b[38;5;241m.\u001b[39mTensor(items)\u001b[38;5;241m.\u001b[39mlong())\n\u001b[0;32m--> 108\u001b[0m HT \u001b[38;5;241m=\u001b[39m trans_to_cuda(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mHT\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mfloat())\n\u001b[1;32m    109\u001b[0m node_masks \u001b[38;5;241m=\u001b[39m trans_to_cuda(torch\u001b[38;5;241m.\u001b[39mTensor(node_masks)\u001b[38;5;241m.\u001b[39mfloat())\n\u001b[1;32m    110\u001b[0m node \u001b[38;5;241m=\u001b[39m model(items, HT)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "model = trans_to_cuda(DocumentGraph(pre_trained_weight, class_weights, len(vocab_dic)+1, len(labels_dic)))\n",
    "\n",
    "for epoch in range(20):\n",
    "    print('-------------------------------------------------------')\n",
    "    print('epoch: ', epoch)\n",
    "\n",
    "    train_model(model, train_data)\n",
    "\n",
    "    valid_detail, valid_acc = test_model(model, valid_data, False)\n",
    "    detail, acc = test_model(model, test_data, False)\n",
    "    print('Validation Accuracy:\\t%.4f, Test Accuracy:\\t%.4f'% (valid_acc,acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c4bc7c-906b-4b78-9e54-6806870268b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
